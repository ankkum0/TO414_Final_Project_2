---
title: "TO414_Final_Project_2"
author: "Group 5: The M.A.A.A.D. Koderz (Matthew Burger, Andrew Schlitter, Ankita Kumar, Ashton Howard, Daphne Fabre, Kushaal Sharma)"
date: "2025-11-18"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    code_folding: show # Shows all code chunks by default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prediciting Whether Job Postings are Fradulent 

## Executive Summary

## Step 0: What's the point?

### Situation:
The rise of ghost and fraudulent job postings has become a major problem for job platforms, with recent industry reports estimating that up to **20–30%** of online job ads show signs of suspicious or deceptive activity. These posts create harmful experiences for users: wasting applicants’ time, exposing them to phishing attempts, and eroding trust in the platform. They also hurt employers, who depend on credible marketplaces to attract qualified candidates. For job-posting websites like LinkedIn or Indeed, the challenge is scale: **millions of posts go live every month**, making **manual review impossible**. Predictive analytics offers a way to proactively identify high-risk postings by learning patterns that distinguish legitimate jobs from fraudulent or “ghost” listings (those that are never actually reviewed or filled). 
### Data Issues:


### Goal:


### Models:
We will be creating a **logistic regression, decision tree, SVM, random forest, KNN, and ANN model**. We will also be created a **stacked model** that combines this individual models. This will be accomplished using a decision tree model (as the second level model put on top of these other individual models). We want to use a decision tree model as it will combine these models without aggregating. When we aggregate models, we will get a model that in an average of the models (it will not be better than the best model). With the decision tree, the goal is to take the best of each model to produce a superior model that outperforms the individual models.

### Outcome:
By building and comparing models such as logistic regression, decision tree, SVM, random forest, KNN, ANN, and stacked models, we can evaluate which approach most effectively reduces false negatives, because missing a fraudulent post is far more damaging than mistakenly flagging a real one. **Ultimately, the goal of this project is to design a model that improves platform safety, protects job seekers, and strengthens the integrity of online hiring ecosystems.**

## Step 1: Load Data
We need to load the data into an object (`job`) so that we can interact with it. We don't want to set `stringsAsFactors` to true, since there are some string data that should not be converted to factors that we will need to deal with while cleaning the data.

```{r}
# Let's store the data in the job object so we can interact with it
job <- read.csv("fake_job_postings.csv") 
```

## Step 2: Clean Data
We need get a sense of the data, before we can start cleaning it. It is important to remove any columns that are unnecessary or would be data we should not use in our prediction models (e.g., data that would not be available to make predictions). It is also a good time to deal with NA data (if there are any). Also, for KNN and ANN model, it is important to dummify and scale the data. So, we will use `job` for the logistic regression, SVM, random forest, and decision tree models and `job_scaled` for the KNN and ANN models. After cleaning the data, it is good to double check the data to ensure the desired results are achieved. 

### Explore Data
```{r}
str(job) # Let's get a sense of columns and data types
summary(job)
```

### Modify Data (Change and Delete)
We want to remove columns that are irrelevant or that we will not have access to when using this model to predict the fraudulent outcome status. We also want to be careful about how we modify data as changing data into factors or other simplifications can result in information being lost (which will hurt the robustness of our models).

```{r}
job$job_id <- NULL # This is not necessary, so let's delete it
job$title <- nchar(job$title) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$location <- as.factor(job$location)
job$department <- as.factor(job$department)
job$salary_range <- as.factor(job$salary_range)
job$company_profile <- nchar(job$company_profile) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$description <- nchar(job$description) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$requirements <- nchar(job$requirements) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$benefits <- nchar(job$benefits) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$employment_type <- as.factor(job$employment_type)
job$required_experience <- as.factor(job$required_experience)
job$required_education <- as.factor(job$required_education)
job$industry <- as.factor(job$industry)
job$function. <- as.factor(job$function.)
job$benefits <- ifelse(is.na(job$benefits), mean(job$benefits, na.rm = T), job$benefits)
```

```{r}
# With 84% of the values being null, we figured it best to transform the data into binary values of whether salary is known or not. It did not make sense to try to replace NA values with the mean since we only have values for 16% of the data. 
# Count blanks
sum(job$salary_range == "", na.rm = TRUE)

job$salary_known <- ifelse(is.na(job$salary_range) | job$salary_range == "", 0, 1)
job$salary_known <- factor(job$salary_known)
table(job$salary_known)

```

### Department Cleaning

```{r}
# 1️⃣ Convert to character, trim, lowercase
job$department_clean <- tolower(trimws(as.character(job$department)))

# 2️⃣ Replace blanks or NAs with "NA"
job$department_clean[job$department_clean == "" | is.na(job$department_clean)] <- "NA"

# 3️⃣ Merge obvious duplicates / similar departments
merge_list <- list(
  "customer service" = c("customer service", "customer service ", "cs", "csd relay"),
  "it"               = c("it", "information technology", "it services"),
  "marketing"        = c("marketing", "performance marketing"),
  "sales"            = c("sales", "sales and marketing"),
  "administration"   = c("admin", "administrative", "administration", "administration support", "admin/clerical", "admin - clerical"),
  "accounting"       = c("accounting", "accounting/finance", "accounting and finance", "accounting & finance", "accounting/payroll"),
  "engineering"      = c("engineering", "engineering "),
  "hr"               = c("hr", "human resources"),
  "product"          = c("product", "product development", "product innovation", "product team"),
  "operations"       = c("operations", "oil & energy", "oil and gas", "maintenance"),
  "customer_facing"        = c("client services", "customer success", "customer support", "content", "creative services"),
  "business_management"    = c("business", "business development", "management", "project management"),
  "tech_development"       = c("software development", ".net", ".net development", "tech", "technical", "technical support", "design", "development"),
  "education_training"     = c("didactics", "education", "editorial"),
  "operations_logistics"   = c("warehouse", "voyageur medical transportation")
)

# Apply merges
for (new_name in names(merge_list)) {
  job$department_clean[job$department_clean %in% merge_list[[new_name]]] <- new_name
}

# 4️⃣ Group rare departments (≤10 occurrences) into "other"
dept_counts <- table(job$department_clean)
job$department_clean <- ifelse(dept_counts[job$department_clean] > 10,
                               job$department_clean,
                               "other")

# 5️⃣ Convert to factor
job$department_clean <- factor(job$department_clean)

# Check resulting counts
table(job$department_clean)

```

### Industry Clean

```{r}
# Convert to character and trim whitespace
job$industry_clean <- trimws(as.character(job$industry))

# Replace blanks or NA with "NA"
job$industry_clean[job$industry_clean == "" | is.na(job$industry_clean)] <- "NA"

# Create industry groupings
industry_map <- list(
  "Technology & Software" = c(
    "Computer Software", "Information Technology and Services", "Internet",
    "Computer Games", "Computer Hardware", "Computer Networking", 
    "Computer & Network Security", "Semiconductors", "Information Services",
    "Program Development", "Nanotechnology"
  ),
  "Healthcare, Wellness & Life Sciences" = c(
    "Healthcare, Wellness & Life Sciences", "Hospital & Health Care",
    "Medical Practice", "Mental Health Care", "Health, Wellness and Fitness",
    "Pharmaceuticals", "Biotechnology", "Medical Devices", "Veterinary"
  ),
  "Finance, Banking & Insurance" = c(
    "Financial Services", "Banking", "Insurance", "Investment Management",
    "Venture Capital & Private Equity", "Capital Markets", "Investment Banking",
    "Accounting"
  ),
  "Business Administration" = c(
    "Staffing and Recruiting", "Human Resources", "Executive Office"
  ),
  "Consulting, Professional Services & Legal" = c(
    "Management Consulting", "Legal Services", "Law Practice", "Government Relations",
    "Alternative Dispute Resolution", "Individual & Family Services"
  ),
  "Consumer Goods, Retail & Fashion" = c(
    "Consumer Goods", "Consumer Services", "Retail", "Apparel & Fashion",
    "Cosmetics", "Sporting Goods", "Luxury Goods & Jewelry", "Textiles",
    "Furniture", "Consumer Electronics", "Wholesale"
  ),
  "Media, Entertainment & Creative" = c(
    "Public Relations and Communications", "Media Production", "Broadcast Media",
    "Publishing", "Music", "Entertainment", "Animation", "Graphic Design",
    "Design", "Photography", "Writing and Editing", "Motion Pictures and Film",
    "Market Research", "Online Media", "Performing Arts", "Sports",
    "Marketing and Advertising"
  ),
  "Hospitality, Travel & Leisure" = c(
    "Hospitality", "Leisure, Travel & Tourism", "Restaurants", "Gambling & Casinos",
    "Airlines/Aviation", "Events Services", "Facilities Services"
  ),
  "Education & Training" = c(
    "Education Management", "E-Learning", "Primary/Secondary Education", 
    "Higher Education", "Professional Training & Coaching", "Libraries",
    "Museums and Institutions", "Translation and Localization", "Research"
  ),
  "Manufacturing & Industrial" = c(
    "Electrical/Electronic Manufacturing", "Mechanical or Industrial Engineering",
    "Industrial Automation", "Machinery", "Chemicals", "Plastics",
    "Printing", "Packaging and Containers", "Shipbuilding", "Civil Engineering",
    "Automotive", "Business Supplies and Equipment"
  ),
  "Energy, Utilities & Environment" = c(
    "Oil & Energy", "Renewables & Environment", "Utilities", "Environmental Services",
    "Mining & Metals", "Wireless", "Telecommunications"
  ),
  "Transportation, Logistics & Supply Chain" = c(
    "Logistics and Supply Chain", "Warehousing", "Transportation/Trucking/Railroad",
    "Package/Freight Delivery", "Maritime", "Import and Export",
    "International Trade and Development", "Outsourcing/Offshoring"
  ),
  "Agriculture, Food & Natural Resources" = c(
    "Food & Beverages", "Food Production", "Farming", "Fishery", "Ranching",
    "Wine and Spirits"
  ),
  "Real Estate & Construction" = c(
    "Construction", "Real Estate", "Commercial Real Estate", "Building Materials",
    "Architecture & Planning"
  ),
  "Government, Nonprofit & Public Sector" = c(
    "Government Administration", "Nonprofit Organization Management",
    "Civic & Social Organization", "Public Policy", "Public Safety",
    "Law Enforcement", "Philanthropy", "Fund-Raising", "Religious Institutions"
  ),
  "Defense, Security & Aerospace" = c(
    "Defense & Space", "Military", "Security and Investigations", "Aviation & Aerospace"
  )
)

# Apply the mapping
for (group in names(industry_map)) {
  job$industry_clean[job$industry_clean %in% industry_map[[group]]] <- group
}

# Optional: group any remaining very rare industries (≤10 occurrences) into "other"
industry_counts <- table(job$industry_clean)
job$industry_clean <- ifelse(industry_counts[job$industry_clean] > 10,
                             job$industry_clean,
                             "other")

# Convert to factor
job$industry_clean <- factor(job$industry_clean)

# Check resulting counts
table(job$industry_clean)

```
### Function Cleaning

```{r}
# Convert to character and trim whitespace
job$function_clean <- trimws(as.character(job$function.))  # assuming your column is 'function.'

# Replace blanks or NA with "NA"
job$function_clean[job$function_clean == "" | is.na(job$function_clean)] <- "NA"

# Create function groupings
function_map <- list(
  "Marketing & Advertising" = c("Marketing & Advertising", "Marketing", "Advertising", "Public Relations"),
  "Analytics & Business Development" = c("Business Development", "Data Analyst", "Business Analyst"),
  "Sales & Customer Service & IT" = c("Customer Service", "Sales", "Information Technology"),
  "Management & Leadership" = c("Management", "Strategy/Planning", "General Business", "Administrative"),
  "Engineering & Production" = c("Engineering", "Production", "Manufacturing", "Product & Project", "Product Management", "Project Management"),
  "Healthcare & Science" = c("Health Care Provider", "Science"),
  "Supply Chain & Logistics" = c("Supply Chain", "Purchasing", "Distribution"),
  "Finance & Accounting" = c("Finance", "Accounting/Auditing", "Financial Analyst"),
  "Human Resources & Training" = c("Human Resources", "Training", "Consulting"),
  "Legal & Compliance" = c("Legal", "Quality Assurance"),
  "Arts" = c("Art/Creative", "Writing/Editing", "Design")
)

# Apply the mapping
for (group in names(function_map)) {
  job$function_clean[job$function_clean %in% function_map[[group]]] <- group
}

# Optional: group any remaining very rare functions into "other"
function_counts <- table(job$function_clean)
job$function_clean <- ifelse(function_counts[job$function_clean] > 10,
                             job$function_clean,
                             "other")

# Convert to factor
job$function_clean <- factor(job$function_clean)

# Check resulting counts
table(job$function_clean)

```



### Location Cleaning

```{r}
# The following columns have too many factor levels (will have to clean up for real model). For this rough model, let's just remove them for now. If a column has too many factor levels it will take the logistic regression model an extremely long time to run. Any column with more than 40 factor levels was removed. 

# Grab the country 
job$loc_country <- substr(job$location, 1, 2)
job$loc_country <- as.factor(job$loc_country)
loc_count <- as.data.frame(table(job$loc_country))

sort(table(job$loc_country))

# tabulate counts by country
tab <- table(job$loc_country)
# map counts back to rows
job$loc_country_val <- as.integer(tab[ as.character(job$loc_country) ])
# create new column: keep country if count > 10, else "Other"
job$loc_country_new <- ifelse(job$loc_country_val > 10, as.character(job$loc_country), "Other")
# Convert to factor first
job$loc_country_new[is.na(job$loc_country_new) | job$loc_country_new == ""] <- "NA"
job$loc_country_new <- factor(job$loc_country_new)
```

### Check Data

```{r}
# We need to remove these columns since we have cleaned them and replaced them with a cleaned version
job$loc_country_val <- NULL
job$loc_country <- NULL
job$department <- NULL
job$salary_range <- NULL
job$industry <- NULL
job$location <- NULL
job$function. <- NULL

str(job)
summary(job)
```

### Dummify and Scale Data for KNN and ANN Models
```{r}
# We can use job for the Logistic Regression, Decision Tree, SVM Models, random forest, and Decision Trees

# For the KNN and ANN models, we need to dummify and scale the data
job_dummy <- as.data.frame(model.matrix(~ . -1, data = job)) 

minmax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

job_scaled <- as.data.frame(lapply(job_dummy, minmax))
```


### Final Data Check
```{r}
# Data for Logistic Regression, Decision Tree, SVM, and Random Forest models
str(job)
summary(job)

# Data for KNN and ANN models
str(job_scaled)
summary(job_scaled)
```


## Step 3: Split Data
After cleaning the data, we can split the data. We will do **70-30 split**. It is important that we split the data so that we can use a portion of the data to train the models and the remaining portion of the data to then test of efficacy of the models created. While we typically do a 50-50 split for the first split when making stacked/two level models, we are doing a 70-30 split in this case because we have almost 18,000 rows of data. There will be enough data to effectively train and test the decision tree model that will combine the six models being created at this point, even if a 70-30 split is used.

```{r}
# Let's do a 70-30 split.

trainprop <- 0.7 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
train_rows <- sample(1:nrow(job), trainprop*nrow(job)) # Get the rows for the training data. We can use train_rows for both job and job_scaled as both data sets have the same number of rows/observations. 

# Train and test data for Logistic Regression, Decision Tree, SVM, and Random Forest Models
job_train <- job[train_rows, ] # Store the training data
job_test <- job[-train_rows, ] # Store the testing data

# Train and test data for KNN and ANN models
job_scaled_train <- job_scaled[train_rows, ] # Store the training data
job_scaled_test <- job_scaled[-train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(job_train$fraudulent)
summary(job_test$fraudulent)

summary(job_scaled_train$fraudulent)
summary(job_scaled_test$fraudulent)

# The mean value is similar between the train and test data sets signifying the split was done successfully
```


## Step 4 & 5: Build a Model + Predict
Now that the data has been split, it is time to create the various models. We will be loading in any necessary libraries, then creating the models based on the training data (`job_train` and `job_scaled_train`). Once the models are trained on the data and then evaluated, we can then use them in the future to predict whether job postings are fraudulent. We will also improve/optimize these models, using different levers based on the model.

### Logistic Regression Model
For the logistic regression model, we can improve the model by adding combinations of predictors and changing the `lr_pred_cutoff` value. We want to extract the log probabilities (`lr_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
# Build Model

# Since we are trying to predict fraudulent, will have that be our response variable. Since we are using all other columns to predict fraudulent, those will be our predictor variables. 

# Let's add some other combinations of predictors to increase the model's accuracy and sensitivity
lr_model <- glm(fraudulent ~ ., data = job_train, family = "binomial")

# Predict

# standard model
lr_pred <- predict(lr_model, job_test, type = "response")
lr_pred_cutoff <- 0.5

lr_bin_pred <- ifelse(lr_pred >= lr_pred_cutoff, 1, 0)

```

### Decision Tree Model
For the decision tree model, we can improve the model by utilizing a `cost_matrix` to change the weighting/ratio between false positive and false negative that the model is trying to balance/optimize. We will also create a decision tree model to use for the stacked model, as we want the results (`dt_pred`) without having placed our thumb on the scale. 

```{r, message=FALSE}
library(C50) # We need this library to run a decision tree model

# Build Model (without weights)
#dt_model <- C5.0(as.factor(fraudulent) ~ ., data = job_train)

dt_model <- C5.0(as.factor(fraudulent) ~ title 
                 + company_profile 
                 + description 
                 + requirements
                 + benefits
                 + telecommuting
                 + has_company_logo
                 + has_questions
                 #+ employment_type #<- causes problem
                 #+ required_experience #<- causes problem
                 #+ required_education #<- causes problem
                 + salary_known
                 + department_clean
                 #+ industry_clean #<- causes problem
                 + function_clean
                 + loc_country_new
                 , data = job_train)

# push through matrix dummy, then push through (janitor - clean names)

plot(dt_model)

dt_model_scaled <- C5.0(as.factor(fraudulent) ~ ., data = job_scaled_train)

plot(dt_model_scaled)

#print(dt_model)

# Predict (without weights)
dt_pred <- predict(dt_model, job_test)

# Build Model (with weights)
cost_matrix <- matrix(c(0, 1, 4, 0), nrow = 2) 

cost_matrix # Check the matrix looks correct 

dt_cost_model <- C5.0(as.factor(fraudulent) ~ title 
                 + company_profile 
                 + description 
                 + requirements
                 + benefits
                 + telecommuting
                 + has_company_logo
                 + has_questions, data = job_train, costs = cost_matrix)

plot(dt_cost_model)

# Predict (with weights)
dt_weights_pred <- predict(dt_cost_model, job_test)
```

### SVM Model
For the SVM model, I can improve the model by changing the kernel. To find the optimal kernel, it will involve a guess-and-check method. I can also improve the model by changing the `SVM_pred_cutoff` value. We want to extract the probabilities (`SVM_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
library(kernlab) # We need this library to run a SVM model

# Build Model
SVM_model <- ksvm(fraudulent ~ ., data = job_train, kernel = "vanilladot")
#SVM_model <- ksvm(fraudulent ~ ., data = job_train, kernel = "vanilladot", prob.model = TRUE) <- switch to once we finish cleaning data

# Predict
SVM_pred <- predict(SVM_model, job_test)
#SVM_pred <- predict(SVM_model, job_test, type = "probabilities") <- switch to once we finish cleaning data

SVM_pred_cutoff <- 0.5

SVM_bin_prob <- ifelse(SVM_pred >= SVM_pred_cutoff, 1, 0)

```

### Random Forest
For the random forest model, we can improve the model by modifying the `ntree` and `nodesize` values.

```{r}
library(randomForest) # We need this library to run a random forest model

# Build Model
#rf_model <- randomForest(as.factor(fraudulent) ~ ., data = job_train, ntree = 2000, nodesize = 5)
#saveRDS(rf_model, "rfJobModel.RDS")
# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

rf_model <- readRDS("rfJobModel.RDS")

varImpPlot(rf_model) # From this plot, we can see that function, company_profile, and description are the biggest predictors of being fraudulent

# Predict
rf_pred <- predict(rf_model, job_test)

```


### KNN Model
For the KNN model, we can improve the model by modifying the `k` value and changing the `KNN_pred_cutoff` value. We want to extract the probabilities (`KNN_prob`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(class)

# Identify predictor columns (all except the target)
predictor_cols <- colnames(job_scaled_train)[colnames(job_scaled_train) != "fraudulent"]

# Train and test predictor matrices
train_X <- job_scaled_train[, predictor_cols]
test_X  <- job_scaled_test[, predictor_cols]

# Target vector
train_y <- job_scaled_train$fraudulent

# Run KNN
KNN_pred <- knn(
  train = train_X,
  test  = test_X,
  cl    = train_y,   # correct: target vector
  k     = 15,
  prob  = TRUE
)

# Convert KNN "prob" attribute to numeric probabilities
KNN_prob <- ifelse(KNN_pred == "1",
                   attr(KNN_pred, "prob"),
                   1 - attr(KNN_pred, "prob"))

# Apply cutoff
KNN_pred_cutoff <- 0.5
KNN_bin_prob <- ifelse(KNN_prob >= KNN_pred_cutoff, 1, 0)

```


```{r}
#library(class) # We need this library to run a KNN model

# Build Model + Predict
#KNN_pred <- knn(train = job_scaled_train[, -100],
#                  test = job_scaled_test[, -100],
#                  cl = job_scaled_train[, -100],
 #                 k = 15, prob = TRUE) 

#KNN_prob <- ifelse(KNN_pred == "1", attr(KNN_pred, "prob"), 1 - attr(KNN_pred, "prob")) #to get most raw data

#KNN_pred_cutoff <- 0.5

#KNN_bin_prob <- ifelse(KNN_prob >= KNN_pred_cutoff, 1, 0)
```

### ANN Model
For the ANN model, we can improve the model by changing the number of nodes (e.g., `hidden = c(5, 3, 2)`) and changing the `ANN_pred_cutoff` value. We want to extract the fractional values (`ANN_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(neuralnet) # We need this library to run a ANN model

# Build Model
set.seed(12345) # Let's make the randomization "not so random"
# ANN_model_1 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8)
# saveRDS(ANN_model_1, "ANNJobModel_1.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

ANN_model_1 <- readRDS("ANNJobModel_1.RDS")

plot(ANN_model_1, rep = "best")

# Predict
ANN_pred <- predict(ANN_model_1, job_scaled_test)
ANN_pred_cutoff <- 0.4 # This is the probability threshold at which we assign the result as 1/positive
ANN_bin_pred <- ifelse(ANN_pred >= ANN_pred_cutoff, 1, 0)

```

## Step 5.5: Combine, Split, Build, & Predict (Stacked Model)
We will now take the prediction results of the various models and create a new data set to build the stacked model off of. Similar to the normal work flow, we will split the data, then use it to the build the 2nd level decision tree model. Finally, we will use the decision tree model produced to predict the test data set. We will also used a cost matrix to optimize the model. 

```{r}
# Combine the predictions of the 6 individual models into a new data frame
stacked_data <- data.frame(
      lr_pred = c(lr_pred),
      dt_pred = c(dt_pred), 
      SVM_pred = c(SVM_pred),
      rf_pred = c(rf_pred),
      KNN_pred = c(KNN_prob), 
      ANN_pred = c(ANN_pred),
      actual = c(job_test$fraudulent)
    )

# Split the data in to train and test data

# Let's do a 50-50 split, again (since there is such a small amount of data). We want there to be a somewhat decent amount of test data
trainprop <- 0.5 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
stacked_train_rows <- sample(1:nrow(stacked_data), trainprop*nrow(stacked_data)) # Get the rows for the training data. We can use train_rows for both churn_data and churn_scaled as both data sets have the same number of rows/observations. 

# Train and test data for the stacked model
stacked_train <- stacked_data[stacked_train_rows, ] # Store the training data
stacked_test <- stacked_data[-stacked_train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(stacked_train$actual)
summary(stacked_test$actual)
# The mean value is similar between the train and test data sets signifying the split was done successfully

# Build and predict a decision tree model as a model stacked on top the other five models 

# Build Model (without weights)
stacked_unweighted_model <- C5.0(as.factor(actual) ~ ., data = stacked_train)
plot(stacked_unweighted_model)

# Build Model (with weights)
stacked_cost_matrix <- matrix(c(0, 1, 3, 0), nrow = 2) 

stacked_cost_matrix # Check the matrix looks correct 

stacked_model <- C5.0(as.factor(actual) ~ ., data = stacked_train, costs = stacked_cost_matrix)
plot(stacked_model)

# Predict (with weights)
stacked_unweighted_pred <- predict(stacked_unweighted_model, stacked_test)

# Predict (with weights)
stacked_pred <- predict(stacked_model, stacked_test)
```

## Step 6: Evaluate Model
Now that the models are created, we can evaluate them by creating confusion matrices. It will be important to look at the accuracy and sensitivity of the model. We also want to make sure that we am minimizing false negatives, as those are much more costly that false positives. [explain what false negatives and false positives are and why false negatives are worse]

```{r class.source = 'fold-hide', message=FALSE}
# Let's build some confusion matrices

library(caret) # We need this library to build a confusion matrix

library(knitr) # Load in library so that the table is formatted in an easy to read manner
```

### Logistic Regression Model
```{r}
cm_lr <- confusionMatrix(as.factor(lr_bin_pred), as.factor(job_test$fraudulent), positive = "1")
cm_lr
```

### Decision Tree Model
```{r}
# Decision Tree Model (without weights)
cm_unweighted_dt <- confusionMatrix(as.factor(dt_pred), as.factor(job_test$fraudulent), positive = "1")
cm_unweighted_dt

# Decision Tree Model (with weights)
cm_dt <- confusionMatrix(as.factor(dt_weights_pred), as.factor(job_test$fraudulent), positive = "1")
cm_dt
```

### SVM Model
```{r}
cm_SVM <- confusionMatrix(as.factor(SVM_bin_prob), as.factor(job_test$fraudulent), positive = "1")
cm_SVM
```

### Random Forest
```{r}
cm_rf <- confusionMatrix(as.factor(rf_pred), as.factor(job_test$fraudulent), positive = "1")
cm_rf
```

### KNN Model
```{r}
cm_KNN <- confusionMatrix(as.factor(KNN_bin_prob), as.factor(job_scaled_test[,72]), positive = "1")
cm_KNN
```

### ANN Model
```{r}
cm_ANN <- confusionMatrix(as.factor(ANN_bin_pred), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN
```

### Stacked Model
```{r}
# Raw data values
cm_unweight_stacked <- confusionMatrix(as.factor(stacked_unweighted_pred), as.factor(stacked_test$actual), positive = "1")
cm_unweight_stacked

cm_stacked <- confusionMatrix(as.factor(stacked_pred), as.factor(stacked_test$actual), positive = "1")
cm_stacked
```
### Model Comparison
```{r class.source = 'fold-hide'}
Model_Comparison <- data.frame(
  "Model" = c("Logistic Regression", "Decision Tree (weights)", "SVM", "Random Forest", "KNN", "ANN", "Stacked Model"),
  "Accuracy" = c(round(cm_lr$overall["Accuracy"], 4), round(cm_dt$overall["Accuracy"], 4), round(cm_SVM$overall["Accuracy"], 4), round(cm_rf$overall["Accuracy"], 4), round(cm_KNN$overall["Accuracy"], 4), round(cm_ANN$overall["Accuracy"], 4), round(cm_stacked$overall["Accuracy"], 4)),
  "Sensitivity" = c(round(cm_lr$byClass["Sensitivity"], 4), round(cm_dt$byClass["Sensitivity"], 4), round(cm_SVM$byClass["Sensitivity"], 4), round(cm_rf$byClass["Sensitivity"], 4), round(cm_KNN$byClass["Sensitivity"], 4), round(cm_ANN$byClass["Sensitivity"], 4), round(cm_stacked$byClass["Sensitivity"], 4)),
  "Kappa" = c(round(cm_lr$overall["Kappa"], 4), round(cm_dt$overall["Kappa"], 4), round(cm_SVM$overall["Kappa"], 4), round(cm_rf$overall["Kappa"], 4), round(cm_KNN$overall["Kappa"], 4), round(cm_ANN$overall["Kappa"], 4), round(cm_stacked$overall["Kappa"], 4)),
  "P-Value" = c(round(cm_lr$overall["AccuracyPValue"], 4), round(cm_dt$overall["AccuracyPValue"], 4), round(cm_SVM$overall["AccuracyPValue"], 4), round(cm_rf$overall["AccuracyPValue"], 4), round(cm_KNN$overall["AccuracyPValue"], 4), round(cm_ANN$overall["AccuracyPValue"], 4), round(cm_stacked$overall["AccuracyPValue"], 4))
)

kable(Model_Comparison, format = "markdown")
```

* The **Random Forest** model has the highest accuracy
* The **Decision Tree** model has the highest sensitivity
* The **Stacked** model has the highest kappa
* The **Random Forest, ANN, and Stacked** models has the smallest p-value

**Stacked Model** <br>
Comparing the stacked model to the individual models,...

## Step 7: Implement Model
Now that the models are created and evaluated, it is time to implement the models and see the financial impacts. It is important to also calculate the financial impact of having no model. I make assumptions (below) for the financial data I am missing. 


### Assumptions

* There are no costs associated with true negatives and true positives
* For **false positives**, these are job postings that our model flags as fraudulent that are actually real. As a result, the company would then have then had to submit a request to get their posting unflagged and placed back on the sight. This would cost a total of **$35**.
  * This would cost $20 in processing the unflagging request
  * This would "cost" $15 in reputation hurt to job posting site as companies do not want to have to post on sites that they have to worry about falsely getting flagged as fraudulent on. 
* For **false negatives**, these are job postings that our model does not flag as fraudulent but actually are. As a result, the site users who apply to this posting risk their time being wasted or even worse, their information being stolen. This would cost a total of **$500**. 
  * This would cost $100 in conducting a formal investigation to remove this post (after the fact) and then investigate any other fraudulent posts related to this on. 
  * This would "cost" $400 in harm to the many hundreds of applicants who applied. If there were 500 applications, it would be about $0.80 per applicant. 

*Note: Results will be scaled up to 100,000 posts so (they are comparable)*


```{r class.source = 'fold-hide'}
# Assumptions
fp_cost <- 35
fn_cost <- 500
num_posts = 100000
nm_scalar = num_posts/nrow(job)
m_scalar = num_posts/nrow(job_test)
sm_scalar = num_posts/nrow(stacked_test)
```


#### No Model
```{r class.source = 'fold-hide'}
nm_frad = sum(job$fraudulent) * nm_scalar
nm_total_cost = nm_frad * fn_cost
```
With no model, there would be no way of flagging fraudulent posts ahead of time. As a result, we end up treating all posts as non-fraudulent. Therefore, we miss **`r format(round(nm_frad, 0), big.mark = ",")` (all) fraudulent posts**, costing **$`r format(round(nm_total_cost, 2), big.mark = ",")`**.

#### Logistic Regression Model
```{r class.source = 'fold-hide'}
lr_fp_cost = fp_cost * cm_lr$table["1", "0"] * m_scalar
lr_fn_cost = fn_cost * cm_lr$table["0", "1"] * m_scalar
lr_total_cost = lr_fp_cost + lr_fn_cost
```
With a model, we ended up with **`r format(round(cm_lr$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(lr_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_lr$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(lr_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(lr_total_cost, 2), big.mark = ",")`**.

#### Decision Tree Model
```{r class.source = 'fold-hide'}
dt_fp_cost = fp_cost * cm_dt$table["1", "0"] * m_scalar
dt_fn_cost = fn_cost * cm_dt$table["0", "1"] * m_scalar
dt_total_cost = dt_fp_cost + dt_fn_cost
```
With a model, we ended up with **`r format(round(cm_dt$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(dt_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_dt$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(dt_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(dt_total_cost, 2), big.mark = ",")`**.

#### SVM Model
```{r class.source = 'fold-hide'}
SVM_fp_cost = fp_cost * cm_SVM$table["1", "0"] * m_scalar
SVM_fn_cost = fn_cost * cm_SVM$table["0", "1"] * m_scalar
SVM_total_cost = SVM_fp_cost + SVM_fn_cost
```
With a model, we ended up with **`r format(round(cm_SVM$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(SVM_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_SVM$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(SVM_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(SVM_total_cost, 2), big.mark = ",")`**.

#### Random Forest
```{r class.source = 'fold-hide'}
rf_fp_cost = fp_cost * cm_rf$table["1", "0"] * m_scalar
rf_fn_cost = fn_cost * cm_rf$table["0", "1"] * m_scalar
rf_total_cost = rf_fp_cost + rf_fn_cost
```
With a model, we ended up with **`r format(round(cm_rf$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(rf_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_rf$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(rf_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(rf_total_cost, 2), big.mark = ",")`**.

#### KNN Model
```{r class.source = 'fold-hide'}
KNN_fp_cost = fp_cost * cm_KNN$table["1", "0"] * m_scalar
KNN_fn_cost = fn_cost * cm_KNN$table["0", "1"] * m_scalar
KNN_total_cost = KNN_fp_cost + KNN_fn_cost
```
With a model, we ended up with **`r format(round(cm_KNN$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(KNN_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_KNN$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(KNN_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(KNN_total_cost, 2), big.mark = ",")`**.

#### ANN Model
```{r class.source = 'fold-hide'}
ANN_fp_cost = fp_cost * cm_ANN$table["1", "0"] * m_scalar
ANN_fn_cost = fn_cost * cm_ANN$table["0", "1"] * m_scalar
ANN_total_cost = ANN_fp_cost + ANN_fn_cost
```
With a model, we ended up with **`r format(round(cm_ANN$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(ANN_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_ANN$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(ANN_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(ANN_total_cost, 2), big.mark = ",")`**.

#### Stacked Model
```{r class.source = 'fold-hide'}
stacked_fp_cost = fp_cost * cm_stacked$table["1", "0"] * sm_scalar
stacked_fn_cost = fn_cost * cm_stacked$table["0", "1"] * sm_scalar
stacked_total_cost = stacked_fp_cost + stacked_fn_cost
```
With a model, we ended up with **`r format(round(cm_stacked$table["1", "0"] * sm_scalar, 0), big.mark = ",")` false positives**, costing **$`r format(round(stacked_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_stacked$table["0", "1"] * sm_scalar, 0), big.mark = ",")` false negatives**, costing **$`r format(round(stacked_fn_cost, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **$`r format(round(stacked_total_cost, 2), big.mark = ",")`**.

### Results
```{r}
results <- data.frame(
  "Model" = c("No Model", "Logistic Regression", "Decision Tree", "SVM", "Random Forest", "KNN", "ANN", "Stacked Model"),
  "Accuracy" = c(0, round(cm_lr$overall["Accuracy"], 4), round(cm_dt$overall["Accuracy"], 4), round(cm_SVM$overall["Accuracy"], 4), round(cm_rf$overall["Accuracy"], 4), round(cm_KNN$overall["Accuracy"], 4), round(cm_ANN$overall["Accuracy"], 4), round(cm_stacked$overall["Accuracy"], 4)),
  "Sensitivity" = c(0, round(cm_lr$byClass["Sensitivity"], 4), round(cm_dt$byClass["Sensitivity"], 4), round(cm_SVM$byClass["Sensitivity"], 4), round(cm_rf$byClass["Sensitivity"], 4), round(cm_KNN$byClass["Sensitivity"], 4), round(cm_ANN$byClass["Sensitivity"], 4), round(cm_stacked$byClass["Sensitivity"], 4))
)
results$FP_Cost <- c(0, lr_fp_cost, dt_fp_cost, SVM_fp_cost, rf_fp_cost, KNN_fp_cost, ANN_fp_cost, stacked_fp_cost)
results$FN_Cost <- c(nm_total_cost, lr_fn_cost, dt_fn_cost, SVM_fn_cost, rf_fn_cost, KNN_fn_cost, ANN_fn_cost, stacked_fn_cost)
results$Total_Cost <- c(nm_total_cost, lr_total_cost, dt_total_cost, SVM_total_cost, rf_total_cost, KNN_total_cost, ANN_total_cost, stacked_total_cost)
results$Cost_Savings <- (nm_total_cost - results$Total_Cost)

results$FP_Cost <- format(round(results$FP_Cost, 2), big.mark = ",")
results$FN_Cost <- format(round(results$FN_Cost, 2), big.mark = ",")
results$Total_Cost <- format(round(results$Total_Cost, 2), big.mark = ",")
results$Cost_Savings <- format(round(results$Cost_Savings, 2), big.mark = ",")

kable(results, format = "markdown", digits = 4)
```


### Conclusion 


# OLD CODE

### KNN

```{r, eval=FALSE}
library(caret)
library(pROC)

set.seed(12345)


if (!exists("job_train") || !exists("job_test")) {
  if (!exists("job")) stop("Provide `job` or predefine `job_train`/`job_test`.")
  idx <- createDataPartition(job$fraudulent, p = 0.7, list = FALSE)
  job_train <- job[idx, ]
  job_test  <- job[-idx, ]
}


to_yes <- function(x) ifelse(x %in% c(1,"1","Yes","TRUE","True",TRUE), "Yes", "No")
job_train$fraudulent <- factor(to_yes(job_train$fraudulent), levels = c("No","Yes"))
job_test$fraudulent  <- factor(to_yes(job_test$fraudulent),  levels = c("No","Yes"))


pred_cols <- setdiff(names(job_train), "fraudulent")
num_cols  <- pred_cols[sapply(job_train[, pred_cols, drop=FALSE], is.numeric)]
if (length(num_cols) == 0) stop("No numeric predictors found. Add some numeric features first.")

x_train <- job_train[, num_cols, drop = FALSE]
y_train <- job_train$fraudulent
x_test  <- job_test[,  num_cols, drop = FALSE]
y_test  <- job_test$fraudulent


ctrl <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)


k_grid <- data.frame(k = c(5, 9, 13, 17, 21))


set.seed(12345)
knn_fit <- train(
  x = x_train,
  y = y_train,
  method = "knn",
  metric = "ROC",
  trControl = ctrl,
  preProcess = c("zv","center","scale"),
  tuneGrid = k_grid
)

print(knn_fit)
plot(knn_fit)  


prob <- predict(knn_fit, newdata = x_test, type = "prob")[, "Yes"]
cls05 <- factor(ifelse(prob >= 0.50, "Yes", "No"), levels = c("No","Yes"))
cls03 <- factor(ifelse(prob >= 0.30, "Yes", "No"), levels = c("No","Yes"))

cm05 <- confusionMatrix(cls05, y_test, positive = "Yes")
cm03 <- confusionMatrix(cls03, y_test, positive = "Yes")

cat("\nConfusion Matrix @ 0.50:\n"); print(cm05)
cat("\nConfusion Matrix @ 0.30:\n"); print(cm03)


roc_obj <- roc(response = y_test, predictor = prob, levels = c("No","Yes"), direction = "<")
cat("\nAUC:\n"); print(auc(roc_obj))
plot(roc_obj, main = "KNN ROC (Numeric predictors)")
```

