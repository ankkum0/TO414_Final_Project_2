---
title: "TO414_Final_Project_2"
author: "Group 5: The M.A.A.A.D. Koderz (Matthew Burger, Andrew Schlitter, Ankita Kumar, Ashton Howard, Daphne Fabre, Kushaal Sharma)"
date: "2025-11-18"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    code_folding: show # Shows all code chunks by default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prediciting Whether Job Postings are Fradulent 

## Executive Summary

## Step 0: What's the point?

### Situation:

### Data Issues:

### Goal:

### Models:

### Outcome:

## Step 1: Load Data
We need to load the data into an object (`job`) so that we can interact with it. We don't want to set `stringsAsFactors` to true, since there are some string data that should not be converted to factors that we will need to deal with while cleaning the data.

```{r}
# Let's store the data in the job object so we can interact with it
job <- read.csv("fake_job_postings.csv") 
```

## Step 2: Clean Data
We need get a sense of the data, before we can start cleaning it. It is important to remove any columns that are unnecessary or would be data we should not use in our prediction models (e.g., data that would not be available to make predictions). It is also a good time to deal with NA data (if there are any). Also, for KNN and ANN model, it is important to dummify and scale the data. So, we will use `job` for the logistic regression, SVM, random forest, and decision tree models and `job_scaled` for the KNN and ANN models. After cleaning the data, it is good to double check the data to ensure the desired results are achieved. 

### Explore Data
```{r}
str(job) # Let's get a sense of columns and data types
summary(job)
```

### Modify Data (Change and Delete)
We want to remove columns that are irrelevant or that we will not have access to when using this model to predict the fraudulent outcome status. We also want to be careful about how we modify data as changing data into factors or other simplifications can result in information being lost (which will hurt the robustness of our models).

```{r}
job$job_id <- NULL # This is not necessary, so let's delete it
job$title <- nchar(job$title) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$location <- as.factor(job$location)
job$department <- as.factor(job$department)
job$salary_range <- as.factor(job$salary_range)
job$company_profile <- nchar(job$company_profile) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$description <- nchar(job$description) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$requirements <- nchar(job$requirements) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$benefits <- nchar(job$benefits) # Since we are doing a rough model, instead of parsing the string, let's just get the length
job$employment_type <- as.factor(job$employment_type)
job$required_experience <- as.factor(job$required_experience)
job$required_education <- as.factor(job$required_education)
job$industry <- as.factor(job$industry)
job$function. <- as.factor(job$function.)
job$benefits <- ifelse(is.na(job$benefits), mean(job$benefits, na.rm = T), job$benefits)
```

### Check Data
```{r}
str(job)
summary(job)

# The following columns have too many factor levels (will have to clean up for real model). For this rough model, let's just remove them for now. If a column has too many factor levels it will take the logistic regression model an extremely long time to run. Any column with more than 40 factor levels was removed. 

# Grab the country - NOT WORKING YET
#job$loc_country <- substr(job$location, 1, 2)
#job$loc_country <- as.factor(job$loc_country)
#loc_count <- as.data.frame(table(job$loc_country))

#sort(table(job$loc_country))

#job$loc_country_val <- loc_count[job$loc_count, 2]
#job$loc_country_new <- ifelse((job$loc_country_val > 10), job_clean$job, "Other")

job$location <- NULL
job$department <- NULL
job$salary_range <- NULL
job$industry <- NULL


```

### Dummify and Scale Data for KNN and ANN Models
```{r}
# We can use job for the Logistic Regression, Decision Tree, SVM Models, random forest, and Decision Trees

# For the KNN and ANN models, we need to dummify and scale the data
job_dummy <- as.data.frame(model.matrix(~ . -1, data = job)) 

minmax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

job_scaled <- as.data.frame(lapply(job_dummy, minmax))
```


### Final Data Check
```{r}
# Data for Logistic Regression, Decision Tree, SVM, and Random Forest models
str(job)
summary(job)

# Data for KNN and ANN models
str(job_scaled)
summary(job_scaled)
```


## Step 3: Split Data
After cleaning the data, we can split the data. We will do **70-30 split**. It is important that we split the data so that we can use a portion of the data to train the models and the remaining portion of the data to then test of efficacy of the models created. While we typically do a 50-50 split for the first split when making stacked/two level models, we are doing a 70-30 split in this case because we have almost 18,000 rows of data. There will be enough data to effectively train and test the decision tree model that will combine the six models being created at this point, even if a 70-30 split is used.

```{r}
# Let's do a 70-30 split.

trainprop <- 0.7 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
train_rows <- sample(1:nrow(job), trainprop*nrow(job)) # Get the rows for the training data. We can use train_rows for both job and job_scaled as both data sets have the same number of rows/observations. 

# Train and test data for Logistic Regression, Decision Tree, SVM, and Random Forest Models
job_train <- job[train_rows, ] # Store the training data
job_test <- job[-train_rows, ] # Store the testing data

# Train and test data for KNN and ANN models
job_scaled_train <- job_scaled[train_rows, ] # Store the training data
job_scaled_test <- job_scaled[-train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(job_train$fraudulent)
summary(job_test$fraudulent)

summary(job_scaled_train$fraudulent)
summary(job_scaled_test$fraudulent)

# The mean value is similar between the train and test data sets signifying the split was done successfully
```

## Step 4 & 5: Build a Model + Predict
Now that the data has been split, it is time to create the various models. We will be loading in any necessary libraries, then creating the models based on the training data (`job_train` and `job_scaled_train`). Once the models are trained on the data and then evaluated, we can then use them in the future to predict whether job postings are fraudulent. We will also improve/optimize these models, using different levers based on the model.

### Logistic Regression Model
For the logistic regression model, we can improve the model by adding combinations of predictors and changing the `lr_pred_cutoff` value. We want to extract the log probabilities (`lr_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
# Build Model

# Since we are trying to predict fraudulent, will have that be our response variable. Since we are using all other columns to predict fraudulent, those will be our predictor variables. 

# Let's add some other combinations of predictors to increase the model's accuracy and sensitivity
lr_model <- glm(fraudulent ~ ., data = job_train, family = "binomial")

# Predict

# standard model
lr_pred <- predict(lr_model, job_test, type = "response")
lr_pred_cutoff <- 0.5

lr_bin_pred <- ifelse(lr_pred >= lr_pred_cutoff, 1, 0)

```

### Decision Tree Model
For the decision tree model, we can improve the model by utilizing a `cost_matrix` to change the weighting/ratio between false positive and false negative that the model is trying to balance/optimize. We will also create a decision tree model to use for the stacked model, as we want the results (`dt_pred`) without having placed our thumb on the scale. 

```{r, message=FALSE}
library(C50) # We need this library to run a decision tree model

# Build Model (without weights)
#dt_model <- C5.0(as.factor(fraudulent) ~ ., data = job_train)

dt_model <- C5.0(as.factor(fraudulent) ~ title 
                 + company_profile 
                 + description 
                 + requirements
                 + benefits
                 + telecommuting
                 + has_company_logo
                 + has_questions
                 #+ employment_type <- causes problem
                 #+ required_experience <- causes problem
                 #+ required_education <- causes problem
                 #+ function.<- causes problem
                 , data = job_train)

# push through matrix dummy, then push through (janintor - clean names)

plot(dt_model)

# Predict (without weights)
dt_pred <- predict(dt_model, job_test)

# Build Model (with weights)
cost_matrix <- matrix(c(0, 1, 4, 0), nrow = 2) 

cost_matrix # Check the matrix looks correct 

dt_cost_model <- C5.0(as.factor(fraudulent) ~ title 
                 + company_profile 
                 + description 
                 + requirements
                 + benefits
                 + telecommuting
                 + has_company_logo
                 + has_questions, data = job_train, costs = cost_matrix)

plot(dt_cost_model)

# Predict (with weights)
dt_weights_pred <- predict(dt_cost_model, job_test)

```

### SVM Model
For the SVM model, I can improve the model by changing the kernel. To find the optimal kernel, it will involve a guess-and-check method. I can also improve the model by changing the `SVM_pred_cutoff` value. We want to extract the probabilities (`SVM_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
library(kernlab) # We need this library to run a SVM model

# Build Model
SVM_model <- ksvm(fraudulent ~ ., data = job_train, kernel = "vanilladot")
#SVM_model <- ksvm(fraudulent ~ ., data = job_train, kernel = "vanilladot", prob.model = TRUE) <- switch to once we finish cleaning data

# Predict
SVM_pred <- predict(SVM_model, job_test)
#SVM_pred <- predict(SVM_model, job_test, type = "probabilities") <- switch to once we finish cleaning data

SVM_pred_cutoff <- 0.5

SVM_bin_prob <- ifelse(SVM_pred >= SVM_pred_cutoff, 1, 0)

```

### Random Forest
For the random forest model, we can improve the model by modifying the `ntree` and `nodesize` values.

```{r}
library(randomForest) # We need this library to run a random forest model

# Build Model
#rf_model <- randomForest(as.factor(fraudulent) ~ ., data = job_train, ntree = 2000, nodesize = 5)
#saveRDS(rf_model, "rfJobModel.RDS")
# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

rf_model <- readRDS("rfJobModel.RDS")

varImpPlot(rf_model) # From this plot, we can see that function, company_profile, and description are the biggest predictors of being fraudulent

# Predict
rf_pred <- predict(rf_model, job_test)

```

### KNN Model
For the KNN model, we can improve the model by modifying the `k` value and changing the `KNN_pred_cutoff` value. We want to extract the probabilities (`KNN_prob`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(class) # We need this library to run a KNN model

# Build Model + Predict
KNN_pred <- knn(train = job_scaled_train[, -72],
                  test = job_scaled_test[, -72],
                  cl = job_scaled_train[, 72],
                  k = 15, prob = TRUE) 

KNN_prob <- ifelse(KNN_pred == "1", attr(KNN_pred, "prob"), 1 - attr(KNN_pred, "prob")) #to get most raw data

KNN_pred_cutoff <- 0.5

KNN_bin_prob <- ifelse(KNN_prob >= KNN_pred_cutoff, 1, 0)
```

### ANN Model
For the ANN model, we can improve the model by changing the number of nodes (e.g., `hidden = c(5, 3, 2)`) and changing the `ANN_pred_cutoff` value. We want to extract the fractional values (`ANN_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(neuralnet) # We need this library to run a ANN model

# Build Model
set.seed(12345) # Let's make the randomization "not so random"
# ANN_model_1 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8)
# saveRDS(ANN_model_1, "ANNJobModel_1.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

ANN_model_1 <- readRDS("ANNJobModel_1.RDS")

plot(ANN_model_1, rep = "best")

# Predict
ANN_pred <- predict(ANN_model_1, job_scaled_test)
ANN_pred_cutoff <- 0.4 # This is the probability threshold at which we assign the result as 1/positive
ANN_bin_pred <- ifelse(ANN_pred >= ANN_pred_cutoff, 1, 0)

```

## Step 5.5: Combine, Split, Build, & Predict (Stacked Model)
We will now take the prediction results of the various models and create a new data set to build the stacked model off of. Similar to the normal work flow, we will split the data, then use it to the build the 2nd level decision tree model. Finally, we will use the decision tree model produced to predict the test data set. We will also used a cost matrix to optimize the model. 

```{r}
# Combine the predictions of the 6 individual models into a new data frame
stacked_data <- data.frame(
      lr_pred = c(lr_pred),
      dt_pred = c(dt_pred), 
      SVM_pred = c(SVM_pred),
      rf_pred = c(rf_pred),
      KNN_pred = c(KNN_prob), 
      ANN_pred = c(ANN_pred),
      actual = c(job_test$fraudulent)
    )

# Split the data in to train and test data

# Let's do a 50-50 split, again (since there is such a small amount of data). We want there to be a somewhat decent amount of test data
trainprop <- 0.5 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
stacked_train_rows <- sample(1:nrow(stacked_data), trainprop*nrow(stacked_data)) # Get the rows for the training data. We can use train_rows for both churn_data and churn_scaled as both data sets have the same number of rows/observations. 

# Train and test data for the stacked model
stacked_train <- stacked_data[stacked_train_rows, ] # Store the training data
stacked_test <- stacked_data[-stacked_train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(stacked_train$actual)
summary(stacked_test$actual)
# The mean value is similar between the train and test data sets signifying the split was done successfully

# Build and predict a decision tree model as a model stacked on top the other five models 

# Build Model (without weights)
stacked_unweighted_model <- C5.0(as.factor(actual) ~ ., data = stacked_train)
plot(stacked_unweighted_model)

# Build Model (with weights)
stacked_cost_matrix <- matrix(c(0, 1, 3, 0), nrow = 2) 

stacked_cost_matrix # Check the matrix looks correct 

stacked_model <- C5.0(as.factor(actual) ~ ., data = stacked_train, costs = stacked_cost_matrix)
plot(stacked_model)

# Predict (with weights)
stacked_unweighted_pred <- predict(stacked_unweighted_model, stacked_test)

# Predict (with weights)
stacked_pred <- predict(stacked_model, stacked_test)
```

## Step 6: Evaluate Model
Now that the models are created, we can evaluate them by creating confusion matrices. It will be important to look at the accuracy and sensitivity of the model. We also want to make sure that we am minimizing false negatives, as those are much more costly that false positives. [explain what false negatives and false positives are and why false negatives are worse]

```{r class.source = 'fold-hide', message=FALSE}
# Let's build some confusion matrices

library(caret) # We need this library to build a confusion matrix

library(knitr) # Load in library so that the table is formatted in an easy to read manner
```

### Logistic Regression Model
```{r}
cm_lr <- confusionMatrix(as.factor(lr_bin_pred), as.factor(job_test$fraudulent), positive = "1")
cm_lr
```

### Decision Tree Model
```{r}
# Decision Tree Model (without weights)
cm_unweighted_dt <- confusionMatrix(as.factor(dt_pred), as.factor(job_test$fraudulent), positive = "1")
cm_unweighted_dt

# Decision Tree Model (with weights)
cm_dt <- confusionMatrix(as.factor(dt_weights_pred), as.factor(job_test$fraudulent), positive = "1")
cm_dt
```

### SVM Model
```{r}
cm_SVM <- confusionMatrix(as.factor(SVM_bin_prob), as.factor(job_test$fraudulent), positive = "1")
cm_SVM
```

### Random Forest
```{r}
cm_rf <- confusionMatrix(as.factor(rf_pred), as.factor(job_test$fraudulent), positive = "1")
cm_rf
```

### KNN Model
```{r}
cm_KNN <- confusionMatrix(as.factor(KNN_bin_prob), as.factor(job_scaled_test[,72]), positive = "1")
cm_KNN
```

### ANN Model
```{r}
cm_ANN <- confusionMatrix(as.factor(ANN_bin_pred), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN
```

### Stacked Model
```{r}
# Raw data values
cm_unweight_stacked <- confusionMatrix(as.factor(stacked_unweighted_pred), as.factor(stacked_test$actual), positive = "1")
cm_unweight_stacked

cm_stacked <- confusionMatrix(as.factor(stacked_pred), as.factor(stacked_test$actual), positive = "1")
cm_stacked
```
### Model Comparison
```{r class.source = 'fold-hide'}
Model_Comparison <- data.frame(
  "Model" = c("Logistic Regression", "Decision Tree (weights)", "SVM", "Random Forest", "KNN", "ANN", "Stacked Model"),
  "Accuracy" = c(round(cm_lr$overall["Accuracy"], 4), round(cm_dt$overall["Accuracy"], 4), round(cm_SVM$overall["Accuracy"], 4), round(cm_rf$overall["Accuracy"], 4), round(cm_KNN$overall["Accuracy"], 4), round(cm_ANN$overall["Accuracy"], 4), round(cm_stacked$overall["Accuracy"], 4)),
  "Sensitivity" = c(round(cm_lr$byClass["Sensitivity"], 4), round(cm_dt$byClass["Sensitivity"], 4), round(cm_SVM$byClass["Sensitivity"], 4), round(cm_rf$byClass["Sensitivity"], 4), round(cm_KNN$byClass["Sensitivity"], 4), round(cm_ANN$byClass["Sensitivity"], 4), round(cm_stacked$byClass["Sensitivity"], 4)),
  "Kappa" = c(round(cm_lr$overall["Kappa"], 4), round(cm_dt$overall["Kappa"], 4), round(cm_SVM$overall["Kappa"], 4), round(cm_rf$overall["Kappa"], 4), round(cm_KNN$overall["Kappa"], 4), round(cm_ANN$overall["Kappa"], 4), round(cm_stacked$overall["Kappa"], 4)),
  "P-Value" = c(round(cm_lr$overall["AccuracyPValue"], 4), round(cm_dt$overall["AccuracyPValue"], 4), round(cm_SVM$overall["AccuracyPValue"], 4), round(cm_rf$overall["AccuracyPValue"], 4), round(cm_KNN$overall["AccuracyPValue"], 4), round(cm_ANN$overall["AccuracyPValue"], 4), round(cm_stacked$overall["AccuracyPValue"], 4))
)

kable(Model_Comparison, format = "markdown")
```

* The **Random Forest** model has the highest accuracy
* The **Decision Tree** model has the highest sensitivity
* The **Stacked** model has the highest kappa
* The **Random Forest, ANN, and Stacked** models has the smallest p-value

**Stacked Model** <br>
Comparing the stacked model to the individual models,...

## Step 7: Implement Model
Now that the models are created and evaluated, it is time to implement the models and see the financial impacts. It is important to also calculate the financial impact of having no model. I make assumptions (below) for the financial data I am missing. 


### Assumptions

```{r class.source = 'fold-hide'}
# Assumptions

```


#### No Model
```{r class.source = 'fold-hide'}

```
With no model, ...

#### Logistic Regression Model
```{r class.source = 'fold-hide'}

```
With a model, ...

#### Decision Tree Model
```{r class.source = 'fold-hide'}

```
With a model, ...

#### SVM Model
```{r class.source = 'fold-hide'}

```
With a model, ...

#### Random Forest
```{r class.source = 'fold-hide'}

```
With a model, ...

#### KNN Model
```{r class.source = 'fold-hide'}

```
With a model, ...

#### ANN Model
```{r class.source = 'fold-hide'}

```
With a model, ...

#### Stacked Model
```{r class.source = 'fold-hide'}

```
With a model, ...


### Results
```{r}

# kable(results, format = "markdown", digits = 4)
```


### Conclusion 


# OLD CODE

### KNN

```{r, eval=FALSE}
library(caret)
library(pROC)

set.seed(12345)


if (!exists("job_train") || !exists("job_test")) {
  if (!exists("job")) stop("Provide `job` or predefine `job_train`/`job_test`.")
  idx <- createDataPartition(job$fraudulent, p = 0.7, list = FALSE)
  job_train <- job[idx, ]
  job_test  <- job[-idx, ]
}


to_yes <- function(x) ifelse(x %in% c(1,"1","Yes","TRUE","True",TRUE), "Yes", "No")
job_train$fraudulent <- factor(to_yes(job_train$fraudulent), levels = c("No","Yes"))
job_test$fraudulent  <- factor(to_yes(job_test$fraudulent),  levels = c("No","Yes"))


pred_cols <- setdiff(names(job_train), "fraudulent")
num_cols  <- pred_cols[sapply(job_train[, pred_cols, drop=FALSE], is.numeric)]
if (length(num_cols) == 0) stop("No numeric predictors found. Add some numeric features first.")

x_train <- job_train[, num_cols, drop = FALSE]
y_train <- job_train$fraudulent
x_test  <- job_test[,  num_cols, drop = FALSE]
y_test  <- job_test$fraudulent


ctrl <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)


k_grid <- data.frame(k = c(5, 9, 13, 17, 21))


set.seed(12345)
knn_fit <- train(
  x = x_train,
  y = y_train,
  method = "knn",
  metric = "ROC",
  trControl = ctrl,
  preProcess = c("zv","center","scale"),
  tuneGrid = k_grid
)

print(knn_fit)
plot(knn_fit)  


prob <- predict(knn_fit, newdata = x_test, type = "prob")[, "Yes"]
cls05 <- factor(ifelse(prob >= 0.50, "Yes", "No"), levels = c("No","Yes"))
cls03 <- factor(ifelse(prob >= 0.30, "Yes", "No"), levels = c("No","Yes"))

cm05 <- confusionMatrix(cls05, y_test, positive = "Yes")
cm03 <- confusionMatrix(cls03, y_test, positive = "Yes")

cat("\nConfusion Matrix @ 0.50:\n"); print(cm05)
cat("\nConfusion Matrix @ 0.30:\n"); print(cm03)


roc_obj <- roc(response = y_test, predictor = prob, levels = c("No","Yes"), direction = "<")
cat("\nAUC:\n"); print(auc(roc_obj))
plot(roc_obj, main = "KNN ROC (Numeric predictors)")
```

