---
title: "TO414_Final_Project_2"
author: "Group 5: The M.A.A.A.D. Koderz (Matthew Burger, Andrew Schlitter, Ankita Kumar, Ashton Howard, Daphne Fabre, Kushaal Sharma)"
date: "2025-11-18"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    code_folding: show # Shows all code chunks by default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prediciting Whether Job Postings are Fradulent 

## Executive Summary

## Step 0: What's the point?

### Situation:
The rise of ghost and fraudulent job postings has become a major problem for job platforms, with recent industry reports estimating that up to **20–30%** of online job ads show signs of suspicious or deceptive activity. These posts create harmful experiences for users: wasting applicants’ time, exposing them to phishing attempts, and eroding trust in the platform. They also hurt employers, who depend on credible marketplaces to attract qualified candidates. For job-posting websites like LinkedIn or Indeed, the challenge is scale: **millions of posts go live every month**, making **manual review impossible**. Predictive analytics offers a way to proactively identify high-risk postings by learning patterns that distinguish legitimate jobs from fraudulent or “ghost” listings (those that are never actually reviewed or filled). 
### Data Issues:


### Goal:


### Models:
We will be creating a **logistic regression, decision tree, SVM, random forest, KNN, and ANN model**. We will also be created a **stacked model** that combines this individual models. This will be accomplished using a decision tree model (as the second level model put on top of these other individual models). We want to use a decision tree model as it will combine these models without aggregating. When we aggregate models, we will get a model that in an average of the models (it will not be better than the best model). With the decision tree, the goal is to take the best of each model to produce a superior model that outperforms the individual models.

### Outcome:
By building and comparing models such as logistic regression, decision tree, SVM, random forest, KNN, ANN, and stacked models, we can evaluate which approach most effectively reduces false negatives, because missing a fraudulent post is far more damaging than mistakenly flagging a real one. **Ultimately, the goal of this project is to design a model that improves platform safety, protects job seekers, and strengthens the integrity of online hiring ecosystems.**

## Step 1: Load Data
We need to load the data into an object (`job`) so that we can interact with it. We don't want to set `stringsAsFactors` to true, since there are some string data that should not be converted to factors that we will need to deal with while cleaning the data.

```{r}
# Let's store the data in the job object so we can interact with it
job <- read.csv("fake_job_postings.csv") 
```

## Step 2: Clean Data
We need get a sense of the data, before we can start cleaning it. It is important to remove any columns that are unnecessary or would be data we should not use in our prediction models (e.g., data that would not be available to make predictions). It is also a good time to deal with NA data (if there are any). Also, we deal with the variables with too many factors (>40) do not behave well in certain models, so cosolidating them here will be important. Also, for KNN and ANN model, it is important to dummify and scale the data. So, we will use `job` for the logistic regression, SVM, and random forest models, use  `job_dummy` decision tree model (the non-dummified version will not produce a model based on our data) and `job_scaled` for the KNN and ANN models. After cleaning the data, it is good to double check the data to ensure the desired results are achieved. 

### Explore Data
```{r}
str(job) # Let's get a sense of columns and data types
summary(job)
```

### Modify Data (Change and Delete)
We want to remove columns that are irrelevant or that we will not have access to when using this model to predict the fraudulent outcome status. We also want to be careful about how we modify data as changing data into factors or other simplifications can result in information being lost (which will hurt the robustness of our models).

```{r}
job$job_id <- NULL # This is not necessary, so let's delete it

# All of these should be treated like factors instead of strings
job$location <- as.factor(job$location)
job$department <- as.factor(job$department)
job$salary_range <- as.factor(job$salary_range)
job$employment_type <- as.factor(job$employment_type)
job$required_experience <- as.factor(job$required_experience)
job$required_education <- as.factor(job$required_education)
job$industry <- as.factor(job$industry)
job$function. <- as.factor(job$function.)

```

### Benefits Cleaning
Here we parse the words in the benefits columns and transforming them into binary variables. 

```{r}
# Create binary flags for top 5 signals in benefits
job$benefits_pipe <- grepl("\\|", job$benefits)       # pipe symbol
job$benefits_hash <- grepl("#", job$benefits)         # hash symbol
job$benefits_bonus <- grepl("bonus", job$benefits, ignore.case = TRUE)  # keyword: bonus
job$benefits_apply <- grepl("apply|contact", job$benefits, ignore.case = TRUE)  # keywords: apply/contact
job$benefits_benefits <- grepl("benefits", job$benefits, ignore.case = TRUE)  # keyword: benefits

# Convert to numeric 0/1 if needed
job[, c("benefits_pipe", "benefits_hash", "benefits_bonus",
        "benefits_apply", "benefits_benefits")] <- 
  lapply(job[, c("benefits_pipe", "benefits_hash", "benefits_bonus",
                 "benefits_apply", "benefits_benefits")], as.numeric)

job$benefits <- nchar(job$benefits) # Number of characters (length of benefit section) might be beneficial to our prediction model
job$benefits <- ifelse(is.na(job$benefits), mean(job$benefits, na.rm = T), job$benefits) # deal with NAs by replacing with mean value
```



### Title Cleaning
Here we parse the words in the title columns and transforming them into binary variables. 

```{r}
library(dplyr)
library(stringr)

job <- job %>%
  mutate(
    slash_present      = if_else(str_detect(title, "/"), 1, 0),     # slash
    backslash_present  = if_else(str_detect(title, "\\\\"), 1, 0), # backslash
    amp_present        = if_else(str_detect(title, "&"), 1, 0),    # ampersand
    exclam_present     = if_else(str_detect(title, "!"), 1, 0),    # exclamation
    dash_present       = if_else(str_detect(title, "-"), 1, 0),    # dash/hyphen
    multiple_spaces    = if_else(str_detect(title, " {2,}"), 1, 0),# double spaces
    parens_present     = if_else(str_detect(title, "\\(|\\)"), 1, 0), # parentheses
    numbers_present    = if_else(str_detect(title, "[0-9]"), 1, 0) # any digits
  )


job$title <- nchar(job$title) # Number of characters (length of title section) might be beneficial to our prediction model
```

### Requirments Cleaning
Here we parse the words in the requirements columns and transforming them into binary variables. 

```{r}

# Make everything lowercase once for speed
job <- job %>%
  mutate(req_clean = tolower(requirements))

# 1. Requirements missing or very short (< 10 words)
job$req_missing_or_short <- as.integer(
  is.na(job$req_clean) | str_count(job$req_clean, "\\w+") < 10
)

# 2. Heavy engineering / industrial terms
eng_terms <- c(
  "asme", "api", "ansi", "pressure vessel", "heat exchanger", 
  "pumps", "compressor", "valve", "kilovolt", "kv", 
  "scada", "plc", "p&id", "process hazard", "piping"
)

job$has_heavy_engineering_terms <- as.integer(
  str_detect(job$req_clean, str_c(eng_terms, collapse = "|"))
)

# 3. Certifications / accreditations
cert_terms <- c(
  "pmp", "pe", "certified", "license", "licence",
  "six sigma", "cfa", "osha", "hazwoper"
)

job$has_certification_terms <- as.integer(
  str_detect(job$req_clean, str_c(cert_terms, collapse = "|"))
)

# 4. Years of experience (1+, 2+, “years”, “5-10”, etc.)
job$has_years_experience <- as.integer(
  str_detect(job$req_clean, "\\d+\\+?\\s*years?")
)

# 5. Degree required
job$has_degree_required <- as.integer(
  str_detect(job$req_clean, "bachelor|degree|required degree|bs |ms |mba")
)

# 6. Software / tools
tool_terms <- c(
  "ms office", "excel", "word", "powerpoint",
  "sap", "quickbooks", "primavera", "autocad"
)

job$has_tool_software_terms <- as.integer(
  str_detect(job$req_clean, str_c(tool_terms, collapse = "|"))
)

# 7. Safety / regulatory language
safety_terms <- c(
  "osha", "compliance", "safety procedures", "audit",
  "regulations", "hazmat", "hazard"
)

job$has_safety_regulation_terms <- as.integer(
  str_detect(job$req_clean, str_c(safety_terms, collapse = "|"))
)

# 8. Heavy bullet lists / long enumerations
job$req_contains_heavy_lists <- as.integer(
  str_count(job$req_clean, "- |•|\\*|\\n") > 10
)

# 9. Title–requirement mismatch flag
# (Engineering terms inside non-engineering jobs)
job$req_title_mismatch <- with(job, as.integer(
  str_detect(req_clean, str_c(eng_terms, collapse = "|")) &
    !str_detect(tolower(title), "engineer|technician|operator|mechanic")
))

job$req_clean <- NULL # No longer need this
job$requirements <- nchar(job$requirements) # Number of characters (length of requirement section) might be beneficial to our prediction model
```


### Description Cleaning
Here we parse the words in the description columns and transforming them into binary variables. 

```{r}
# Create 10 binary feature columns from job$description

job$has_urgent_language <- as.integer(grepl(
  "urgent|apply now|immediate start|asap|start immediately",
  job$description, ignore.case = TRUE))

job$has_no_experience_needed <- as.integer(grepl(
  "no experience|training provided|any background|anyone can apply",
  job$description, ignore.case = TRUE))

job$has_salary_info <- as.integer(grepl(
  "\\$|salary|per hour|per annum|k",   # detects any salary mention
  job$description, ignore.case = TRUE))

job$has_qualification_terms <- as.integer(grepl(
  "bachelor|degree|certificate|qualification|experience required",
  job$description, ignore.case = TRUE))

job$has_benefits_stated <- as.integer(grepl(
  "benefits|health insurance|401k|superannuation|paid time off|leave",
  job$description, ignore.case = TRUE))

job$has_technical_terms <- as.integer(grepl(
  "sql|python|excel|jira|crm|compliance|financial analysis",
  job$description, ignore.case = TRUE))

job$has_contact_number_or_whatsapp <- as.integer(grepl(
  "\\b\\d{3}[- ]?\\d{3}[- ]?\\d{4}\\b|whatsapp",
  job$description, ignore.case = TRUE))

job$has_company_language <- as.integer(grepl(
  "team|mission|vision|culture|values|our company",
  job$description, ignore.case = TRUE))

job$has_commission_only_language <- as.integer(grepl(
  "commission only|high earning|earn up to|unlimited income",
  job$description, ignore.case = TRUE))

job$description <- nchar(job$description) # Number of characters (length of description section) might be beneficial to our prediction model
```


### Company Profile Cleaning
Here we parse the words in the company profile columns and transforming them into binary variables. 

```{r}
library(dplyr)
library(stringr)

job <- job %>%
  mutate(
    has_referral_bonus = ifelse(str_detect(tolower(company_profile), "referral bonus|bonus for referral"), 1, 0),
    has_signing_bonus  = ifelse(str_detect(tolower(company_profile), "signing bonus|bonus by"), 1, 0),
    has_perks          = ifelse(str_detect(tolower(company_profile), "perks|corporate discounts|benefits"), 1, 0),
    has_relocation     = ifelse(str_detect(tolower(company_profile), "relocation|out of town candidates|move assistance"), 1, 0)
  )

# Preview
#job %>% select(company_profile, has_referral_bonus, has_signing_bonus, has_perks, has_relocation) %>% head()
job$company_profile <- nchar(job$company_profile) # Number of characters (length of company profile section) might be beneficial to our prediction model
```

### Salary Range Cleaning

```{r}
# With 84% of the values being null, we figured it best to transform the data into binary values of whether salary is known or not. It did not make sense to try to replace NA values with the mean since we only have values for 16% of the data. 
# Count blanks
sum(job$salary_range == "", na.rm = TRUE)

job$salary_known <- ifelse(is.na(job$salary_range) | job$salary_range == "", 0, 1)
job$salary_known <- factor(job$salary_known)
table(job$salary_known)

```

### Department Cleaning

```{r}
# 1️⃣ Convert to character, trim, lowercase
job$department_clean <- tolower(trimws(as.character(job$department)))

# 2️⃣ Replace blanks or NAs with "NA"
job$department_clean[job$department_clean == "" | is.na(job$department_clean)] <- "NA"

# 3️⃣ Merge obvious duplicates / similar departments
merge_list <- list(
  "customer service" = c("customer service", "customer service ", "cs", "csd relay"),
  "it"               = c("it", "information technology", "it services"),
  "marketing"        = c("marketing", "performance marketing"),
  "sales"            = c("sales", "sales and marketing"),
  "administration"   = c("admin", "administrative", "administration", "administration support", "admin/clerical", "admin - clerical"),
  "accounting"       = c("accounting", "accounting/finance", "accounting and finance", "accounting & finance", "accounting/payroll"),
  "engineering"      = c("engineering", "engineering "),
  "hr"               = c("hr", "human resources"),
  "product"          = c("product", "product development", "product innovation", "product team"),
  "operations"       = c("operations", "oil & energy", "oil and gas", "maintenance"),
  "customer_facing"        = c("client services", "customer success", "customer support", "content", "creative services"),
  "business_management"    = c("business", "business development", "management", "project management"),
  "tech_development"       = c("software development", ".net", ".net development", "tech", "technical", "technical support", "design", "development"),
  "education_training"     = c("didactics", "education", "editorial"),
  "operations_logistics"   = c("warehouse", "voyageur medical transportation")
)

# Apply merges
for (new_name in names(merge_list)) {
  job$department_clean[job$department_clean %in% merge_list[[new_name]]] <- new_name
}

# 4️⃣ Group rare departments (≤10 occurrences) into "other"
dept_counts <- table(job$department_clean)
job$department_clean <- ifelse(dept_counts[job$department_clean] > 10,
                               job$department_clean,
                               "other")

# 5️⃣ Convert to factor
job$department_clean <- factor(job$department_clean)

# Check resulting counts
table(job$department_clean)

```

### Industry Clean

```{r}
# Convert to character and trim whitespace
job$industry_clean <- trimws(as.character(job$industry))

# Replace blanks or NA with "NA"
job$industry_clean[job$industry_clean == "" | is.na(job$industry_clean)] <- "NA"

# Create industry groupings
industry_map <- list(
  "Technology & Software" = c(
    "Computer Software", "Information Technology and Services", "Internet",
    "Computer Games", "Computer Hardware", "Computer Networking", 
    "Computer & Network Security", "Semiconductors", "Information Services",
    "Program Development", "Nanotechnology"
  ),
  "Healthcare, Wellness & Life Sciences" = c(
    "Healthcare, Wellness & Life Sciences", "Hospital & Health Care",
    "Medical Practice", "Mental Health Care", "Health, Wellness and Fitness",
    "Pharmaceuticals", "Biotechnology", "Medical Devices", "Veterinary"
  ),
  "Finance, Banking & Insurance" = c(
    "Financial Services", "Banking", "Insurance", "Investment Management",
    "Venture Capital & Private Equity", "Capital Markets", "Investment Banking",
    "Accounting"
  ),
  "Business Administration" = c(
    "Staffing and Recruiting", "Human Resources", "Executive Office"
  ),
  "Consulting, Professional Services & Legal" = c(
    "Management Consulting", "Legal Services", "Law Practice", "Government Relations",
    "Alternative Dispute Resolution", "Individual & Family Services"
  ),
  "Consumer Goods, Retail & Fashion" = c(
    "Consumer Goods", "Consumer Services", "Retail", "Apparel & Fashion",
    "Cosmetics", "Sporting Goods", "Luxury Goods & Jewelry", "Textiles",
    "Furniture", "Consumer Electronics", "Wholesale"
  ),
  "Media, Entertainment & Creative" = c(
    "Public Relations and Communications", "Media Production", "Broadcast Media",
    "Publishing", "Music", "Entertainment", "Animation", "Graphic Design",
    "Design", "Photography", "Writing and Editing", "Motion Pictures and Film",
    "Market Research", "Online Media", "Performing Arts", "Sports",
    "Marketing and Advertising"
  ),
  "Hospitality, Travel & Leisure" = c(
    "Hospitality", "Leisure, Travel & Tourism", "Restaurants", "Gambling & Casinos",
    "Airlines/Aviation", "Events Services", "Facilities Services"
  ),
  "Education & Training" = c(
    "Education Management", "E-Learning", "Primary/Secondary Education", 
    "Higher Education", "Professional Training & Coaching", "Libraries",
    "Museums and Institutions", "Translation and Localization", "Research"
  ),
  "Manufacturing & Industrial" = c(
    "Electrical/Electronic Manufacturing", "Mechanical or Industrial Engineering",
    "Industrial Automation", "Machinery", "Chemicals", "Plastics",
    "Printing", "Packaging and Containers", "Shipbuilding", "Civil Engineering",
    "Automotive", "Business Supplies and Equipment"
  ),
  "Energy, Utilities & Environment" = c(
    "Oil & Energy", "Renewables & Environment", "Utilities", "Environmental Services",
    "Mining & Metals", "Wireless", "Telecommunications"
  ),
  "Transportation, Logistics & Supply Chain" = c(
    "Logistics and Supply Chain", "Warehousing", "Transportation/Trucking/Railroad",
    "Package/Freight Delivery", "Maritime", "Import and Export",
    "International Trade and Development", "Outsourcing/Offshoring"
  ),
  "Agriculture, Food & Natural Resources" = c(
    "Food & Beverages", "Food Production", "Farming", "Fishery", "Ranching",
    "Wine and Spirits"
  ),
  "Real Estate & Construction" = c(
    "Construction", "Real Estate", "Commercial Real Estate", "Building Materials",
    "Architecture & Planning"
  ),
  "Government, Nonprofit & Public Sector" = c(
    "Government Administration", "Nonprofit Organization Management",
    "Civic & Social Organization", "Public Policy", "Public Safety",
    "Law Enforcement", "Philanthropy", "Fund-Raising", "Religious Institutions"
  ),
  "Defense, Security & Aerospace" = c(
    "Defense & Space", "Military", "Security and Investigations", "Aviation & Aerospace"
  )
)

# Apply the mapping
for (group in names(industry_map)) {
  job$industry_clean[job$industry_clean %in% industry_map[[group]]] <- group
}

# Optional: group any remaining very rare industries (≤10 occurrences) into "other"
industry_counts <- table(job$industry_clean)
job$industry_clean <- ifelse(industry_counts[job$industry_clean] > 10,
                             job$industry_clean,
                             "other")

# Convert to factor
job$industry_clean <- factor(job$industry_clean)

# Check resulting counts
table(job$industry_clean)

```

### Function Cleaning

```{r}
# Convert to character and trim whitespace
job$function_clean <- trimws(as.character(job$function.))  # assuming your column is 'function.'

# Replace blanks or NA with "NA"
job$function_clean[job$function_clean == "" | is.na(job$function_clean)] <- "NA"

# Create function groupings
function_map <- list(
  "Marketing & Advertising" = c("Marketing & Advertising", "Marketing", "Advertising", "Public Relations"),
  "Analytics & Business Development" = c("Business Development", "Data Analyst", "Business Analyst"),
  "Sales & Customer Service & IT" = c("Customer Service", "Sales", "Information Technology"),
  "Management & Leadership" = c("Management", "Strategy/Planning", "General Business", "Administrative"),
  "Engineering & Production" = c("Engineering", "Production", "Manufacturing", "Product & Project", "Product Management", "Project Management"),
  "Healthcare & Science" = c("Health Care Provider", "Science"),
  "Supply Chain & Logistics" = c("Supply Chain", "Purchasing", "Distribution"),
  "Finance & Accounting" = c("Finance", "Accounting/Auditing", "Financial Analyst"),
  "Human Resources & Training" = c("Human Resources", "Training", "Consulting"),
  "Legal & Compliance" = c("Legal", "Quality Assurance"),
  "Arts" = c("Art/Creative", "Writing/Editing", "Design")
)

# Apply the mapping
for (group in names(function_map)) {
  job$function_clean[job$function_clean %in% function_map[[group]]] <- group
}

# Optional: group any remaining very rare functions into "other"
function_counts <- table(job$function_clean)
job$function_clean <- ifelse(function_counts[job$function_clean] > 10,
                             job$function_clean,
                             "other")

# Convert to factor
job$function_clean <- factor(job$function_clean)

# Check resulting counts
table(job$function_clean)

```

### Location Cleaning

```{r}

# Grab the country 
job$loc_country <- substr(job$location, 1, 2)
job$loc_country <- as.factor(job$loc_country)
loc_count <- as.data.frame(table(job$loc_country))

sort(table(job$loc_country))

# tabulate counts by country
tab <- table(job$loc_country)
# map counts back to rows
job$loc_country_val <- as.integer(tab[ as.character(job$loc_country) ])
# create new column: keep country if count > 10, else "Other"
job$loc_country_new <- ifelse(job$loc_country_val > 10, as.character(job$loc_country), "Other")
# Convert to factor first
job$loc_country_new[is.na(job$loc_country_new) | job$loc_country_new == ""] <- "NA"
job$loc_country_new <- factor(job$loc_country_new)
```

### Check Data

```{r}
# We need to remove these columns since we have cleaned them and replaced them with a cleaned version
job$loc_country_val <- NULL
job$loc_country <- NULL
job$department <- NULL
job$salary_range <- NULL
job$industry <- NULL
job$location <- NULL
job$function. <- NULL

str(job)
summary(job)
```

### Dummify and Scale Data for KNN and ANN Models
```{r}
# We can use job for the Logistic Regression, SVM Models, random forest, and Decision Trees

# For the Decision Tree model, we need to dummify the data
job_dummy <- as.data.frame(model.matrix(~ . -1, data = job)) 

minmax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# For the KNN and ANN models, we need to dummify and scale the data
job_scaled <- as.data.frame(lapply(job_dummy, minmax))
```


### Final Data Check
```{r}
# Data for Logistic Regression, SVM, and Random Forest models
str(job)
summary(job)

# Data for Decision Tree Model
str(job_dummy)
summary(job_dummy)

# Data for KNN and ANN models
str(job_scaled)
summary(job_scaled)
```


## Step 3: Split Data
After cleaning the data, we can split the data. We will do **70-30 split**. It is important that we split the data so that we can use a portion of the data to train the models and the remaining portion of the data to then test of efficacy of the models created. While we typically do a 50-50 split for the first split when making stacked/two level models, we are doing a 70-30 split in this case because we have almost 18,000 rows of data. There will be enough data to effectively train and test the decision tree model that will combine the six models being created at this point, even if a 70-30 split is used.

```{r}
# Let's do a 70-30 split.

trainprop <- 0.7 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
train_rows <- sample(1:nrow(job), trainprop*nrow(job)) # Get the rows for the training data. We can use train_rows for both job and job_scaled as both data sets have the same number of rows/observations. 

# Train and test data for Logistic Regression, SVM, and Random Forest Models
job_train <- job[train_rows, ] # Store the training data
job_test <- job[-train_rows, ] # Store the testing data

# Train and test data for Decision Tree Model
job_dummy_train <- job_dummy[train_rows, ] # Store the training data
job_dummy_test <- job_dummy[-train_rows, ] # Store the testing data

# Train and test data for KNN and ANN models
job_scaled_train <- job_scaled[train_rows, ] # Store the training data
job_scaled_test <- job_scaled[-train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(job_train$fraudulent)
summary(job_test$fraudulent)

summary(job_dummy_train$fraudulent)
summary(job_dummy_test$fraudulent)

summary(job_scaled_train$fraudulent)
summary(job_scaled_test$fraudulent)

# The mean value is similar between the train and test data sets signifying the split was done successfully
```


## Step 4 & 5: Build a Model + Predict
Now that the data has been split, it is time to create the various models. We will be loading in any necessary libraries, then creating the models based on the training data (`job_train` and `job_scaled_train`). Once the models are trained on the data and then evaluated, we can then use them in the future to predict whether job postings are fraudulent. We will also improve/optimize these models, using different levers based on the model.

### Logistic Regression Model
For the logistic regression model, we can improve the model by adding combinations of predictors and changing the `lr_pred_cutoff` value. We want to extract the log probabilities (`lr_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
# Build Model

# Since we are trying to predict fraudulent, will have that be our response variable. Since we are using all other columns to predict fraudulent, those will be our predictor variables. 

# Let's add some other combinations of predictors to increase the model's accuracy and sensitivity
# lr_model <- glm(fraudulent ~ . + industry_clean * function_clean
                               #+ description * benefits
                               #+ description * requirements 
                               #+ required_experience * required_education
                               #+ employment_type * required_experience
                               #+ employment_type * required_education
                               #+ required_experience * has_years_experience
                                #, data = job_train, family = "binomial")
# saveRDS(lr_model, "lrJobModel.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

lr_model <- readRDS("lrJobModel.RDS")

# Predictor - Accuracy | Sensitivity
  # Base - 0.9627 | 0.42029
  # industry_clean * function_clean - 0.9623 | 0.44928
  # description * benefits - 0.9625 | 0.45290
  # description * requirements - 0.962 | 0.45652
  # required_experience * required_education - 0.9631 | 0.49275
  # employment_type * required_experience - 0.9618 | 0.49638
  # employment_type * required_education - 0.9532 | 0.52899
  # required_experience * has_years_experience - 0.9629 | 0.52899

# The following combinations of predictors decreased the model's sensitivity and/or accuracy
  #+ required_experience * has_no_experience_needed
  #+ benefits * has_benefits_stated
  #+ benefits * has_referral_bonus
  #+ benefits * has_signing_bonus

# 2nd Logistic Regression Model - using step function to optimize through all combinations of predictors (.*.)
# 2nd model was attempted, however due to number of variables/columns, it took too long

# m1 <- glm(fraudulent ~ . + .*., data = job_train, family = "binomial")
# saveRDS(m1, "LRJobModel_m1.RDS")
# lr_model_2 <- step(m1, direction = "backward") 
# saveRDS(lr_model_2, "LRJobModel_2.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

# lr_model_2 <- readRDS("LRSporModel_2.RDS")

# Predict

# standard model
lr_pred <- predict(lr_model, job_test, type = "response")
lr_pred_cutoff <- 0.5

lr_bin_pred <- ifelse(lr_pred >= lr_pred_cutoff, 1, 0)

```

### Decision Tree Model
For the decision tree model, we can improve the model by utilizing a `cost_matrix` to change the weighting/ratio between false positive and false negative that the model is trying to balance/optimize. We will also create a decision tree model to use for the stacked model, as we want the results (`dt_pred`) without having placed our thumb on the scale. 

```{r, message=FALSE}
library(C50) # We need this library to run a decision tree model

# Build Model (without weights)
# dt_model <- C5.0(as.factor(fraudulent) ~ ., data = job_dummy_train)
# saveRDS(dt_model, "dtJobModel.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

dt_model <- readRDS("dtJobModel.RDS")
plot(dt_model)

# Predict (without weights)
dt_pred <- predict(dt_model, job_dummy_test)

# Build Model (with weights)
cost_matrix <- matrix(c(0, 1, 6, 0), nrow = 2) 
cost_matrix # Check the matrix looks correct 

# dt_cost_model <- C5.0(as.factor(fraudulent) ~ ., data = job_dummy_train, costs = cost_matrix)
# saveRDS(dt_cost_model, "dtJobModel_2.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

dt_cost_model <- readRDS("dtJobModel_2.RDS")
plot(dt_cost_model)

# Predict (with weights)
dt_weights_pred <- predict(dt_cost_model, job_dummy_test)
```

### SVM Model
For the SVM model, I can improve the model by changing the kernel. To find the optimal kernel, it will involve a guess-and-check method. I can also improve the model by changing the `SVM_pred_cutoff` value. We want to extract the probabilities (`SVM_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results. 

```{r}
library(kernlab) # We need this library to run a SVM model

# Build Model
# SVM_model <- ksvm(fraudulent ~ ., data = job_train, kernel = "rbfdot")
# SVM_model_2 <- ksvm(fraudulent ~ ., data = job_train, kernel = "tanhdot")
# saveRDS(SVM_model, "SVMJobModel.RDS")
# saveRDS(SVM_model_2, "SVMJobModel_2.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

SVM_model <- readRDS("SVMJobModel.RDS")
SVM_model_2 <- readRDS("SVMJobModel_2.RDS")

# Kernel - Sensitivity | Accuracy
  # rbfdot - 0.23551 | 0.9605
  # polydot - 0.184783 | 0.9556
  # vanilladot - 0.184783 | 0.9556
  # tanhdot - 0.55072 | 0.5021
  # laplacedot - 0.159420 | 0.9567
  # besseldot - 0.29348 | 0.4746
  # anovadot - 1.00000 | 0.0515
  # splinedot - NA | NA <- no results after letting run for 30 minutes

# From testing all kernels, we can see that the rbfdot had the highest accuracy and 4th highest sensitivity. We can also see that tanhdot produces the highest sensitivity (however, significantly lower accuracy). As a result, we decided to run these two SVM models to see if higher accuracy or sensitivity will result in a better result. 

# Predict
SVM_pred <- predict(SVM_model, job_test)
SVM_pred_2 <- predict(SVM_model_2, job_test)

SVM_pred_cutoff <- 0.1 
# We reduced the cutoff value to 0.1 to reduce the number of false negatives and increase the accuracy and sensitivity of the model.
SVM_pred_2_cutoff <- 0.3
# We reduced the cutoff value to 0.3 to reduce the number of false negatives and increase the accuracy and sensitivity of the model.

SVM_bin_prob <- ifelse(SVM_pred >= SVM_pred_cutoff, 1, 0)
SVM_bin_prob_2 <- ifelse(SVM_pred_2 >= SVM_pred_2_cutoff, 1, 0)

```

### Random Forest
For the random forest model, we can improve the model by modifying the `ntree` and `nodesize` values.

```{r}
library(randomForest) # We need this library to run a random forest model

# Build Model
# rf_model <- randomForest(as.factor(fraudulent) ~ ., data = job_train, ntree = 2000, nodesize = 5)
# saveRDS(rf_model, "rfJobModel.RDS")
# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

rf_model <- readRDS("rfJobModel.RDS")

varImpPlot(rf_model) # From this plot, we can see that function, company_profile, and description are the biggest predictors of being fraudulent

# Predict
rf_pred <- predict(rf_model, job_test)

```


### KNN Model
For the KNN model, we can improve the model by modifying the `k` value and changing the `KNN_pred_cutoff` value. We want to extract the probabilities (`KNN_prob`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(class)

# Identify predictor columns (all except the target)
predictor_cols <- colnames(job_scaled_train)[colnames(job_scaled_train) != "fraudulent"]

# Train and test predictor matrices
train_X <- job_scaled_train[, predictor_cols]
test_X  <- job_scaled_test[, predictor_cols]

# Target vector
train_y <- job_scaled_train$fraudulent

# Run KNN
# KNN_pred <- knn(train = train_X, test = test_X, cl = train_y, k = 4, prob = TRUE) # We optimized k over the range [2, 100] for accuracy and sensitivity
# saveRDS(KNN_pred, "KNNJobModel.RDS")

# k = # | Accuracy | Sensitivity
# k = 100 | 0.9485 | 0.000000
# k = 75 | 0.9485 | 0.000000
# k = 50 | 0.9508 | 0.043478
# k = 40 | 0.9534 | 0.105072
# k = 30 | 0.9603 | 0.25725
# k = 20 | 0.9681 | 0.43478
# k = 10 | 0.9724 | 0.57971
# k = 5 | 0.9754 | 0.65580
# k = 4 | 0.9735 | 0.71377 # highest sensitivity
# k = 3 | 0.9767 | 0.70652 # highest accuracy
# k = 2 | 0.9674 | 0.81522

# We choose k = 4 since k = 4 and k = 3 have similar results. However, the increase in sensitivity from going from 3 to 4 is larger that the decrease in accuracy from 3 to 4. 

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

KNN_pred <- readRDS("KNNJobModel.RDS")

# Convert KNN "prob" attribute to numeric probabilities
KNN_prob <- ifelse(KNN_pred == "1",
                   attr(KNN_pred, "prob"),
                   1 - attr(KNN_pred, "prob"))

# Apply cutoff
KNN_pred_cutoff <- 0.3
# We reduced the cutoff value to 0.3 to reduce the number of false negatives and increase the sensitivity of the model.
KNN_bin_prob <- ifelse(KNN_prob >= KNN_pred_cutoff, 1, 0)

```


```{r}
#library(class) # We need this library to run a KNN model

# Build Model + Predict
#KNN_pred <- knn(train = job_scaled_train[, -100],
#                  test = job_scaled_test[, -100],
#                  cl = job_scaled_train[, -100],
 #                 k = 15, prob = TRUE) 

#KNN_prob <- ifelse(KNN_pred == "1", attr(KNN_pred, "prob"), 1 - attr(KNN_pred, "prob")) #to get most raw data

#KNN_pred_cutoff <- 0.5

#KNN_bin_prob <- ifelse(KNN_prob >= KNN_pred_cutoff, 1, 0)
```

### ANN Model
For the ANN model, we can improve the model by changing the number of nodes (e.g., `hidden = c(5, 3, 2)`) and changing the `ANN_pred_cutoff` value. We want to extract the fractional values (`ANN_pred`) for the stacked model, so that we have the most "raw" version of the predictions/results.

```{r}
library(neuralnet) # We need this library to run a ANN model

# Build Model
set.seed(12345) # Let's make the randomization "not so random"
#ANN_model_1 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8)
#ANN_model_2 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8, hidden = c(3, 2))
#ANN_model_3 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8, hidden = c(5, 3, 2))
#ANN_model_4 <- neuralnet(fraudulent ~ ., data = job_scaled_train, lifesign = "full", stepmax = 1e8, hidden = c(5, 3, 3, 2))
#saveRDS(ANN_model_1, "ANNJobModel_1.RDS")
#saveRDS(ANN_model_2, "ANNJobModel_2.RDS")
#saveRDS(ANN_model_3, "ANNJobModel_3.RDS")
#saveRDS(ANN_model_4, "ANNJobModel_4.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

ANN_model_1 <- readRDS("ANNJobModel_1.RDS")
ANN_model_2 <- readRDS("ANNJobModel_2.RDS")
ANN_model_3 <- readRDS("ANNJobModel_3.RDS")
ANN_model_4 <- readRDS("ANNJobModel_4.RDS")

plot(ANN_model_1, rep = "best")
plot(ANN_model_2, rep = "best")
plot(ANN_model_3, rep = "best")
plot(ANN_model_4, rep = "best")

# Predict
ANN_pred <- predict(ANN_model_1, job_scaled_test)
ANN_pred_cutoff <- 0.3 # We reduced the cutoff value to 0.3 to reduce the number of false negatives and increase the accuracy and sensitivity of the model.
ANN_bin_pred <- ifelse(ANN_pred >= ANN_pred_cutoff, 1, 0)

ANN_pred_2 <- predict(ANN_model_2, job_scaled_test)
ANN_pred_cutoff_2 <- 0.3 # We reduced the cutoff value to 0.3 to increase the sensitivity of the model.
ANN_bin_pred_2 <- ifelse(ANN_pred_2 >= ANN_pred_cutoff_2, 1, 0)

ANN_pred_3 <- predict(ANN_model_3, job_scaled_test)
ANN_pred_cutoff_3 <- 0.2 # We reduced the cutoff value to 0.2 to reduce the number of false negatives and increase the sensitivity of the model.
ANN_bin_pred_3 <- ifelse(ANN_pred_3 >= ANN_pred_cutoff_3, 1, 0)

ANN_pred_4 <- predict(ANN_model_4, job_scaled_test)
ANN_pred_cutoff_4 <- 0.2 # We reduced the cutoff value to 0.2 to reduce the number of false negatives and increase the sensitivity of the model.
ANN_bin_pred_4 <- ifelse(ANN_pred_4 >= ANN_pred_cutoff_4, 1, 0)

```

## Step 5.5: Combine, Split, Build, & Predict (Stacked Model)
We will now take the prediction results of the various models and create a new data set to build the stacked model off of. Similar to the normal work flow, we will split the data, then use it to the build the 2nd level decision tree model. Finally, we will use the decision tree model produced to predict the test data set. We will also used a cost matrix to optimize the model. 

```{r}
# Combine the predictions of the 7 individual models into a new data frame
stacked_data <- data.frame(
      lr_pred = c(lr_pred),
      dt_pred = c(dt_pred), 
      SVM_pred = c(SVM_pred),
      SVM_pred_2 = c(SVM_pred_2),
      rf_pred = c(rf_pred),
      KNN_pred = c(KNN_prob), 
      ANN_pred = c(ANN_pred_3),
      actual = c(job_test$fraudulent)
    )

# Split the data in to train and test data

# Let's do a 50-50 split, again (since there is such a small amount of data). We want there to be a somewhat decent amount of test data
trainprop <- 0.5 # This is the proportion of data we want in our training data set
set.seed(12345) # Let's make the randomization "not so random"
stacked_train_rows <- sample(1:nrow(stacked_data), trainprop*nrow(stacked_data)) # Get the rows for the training data. We can use train_rows for both churn_data and churn_scaled as both data sets have the same number of rows/observations. 

# Train and test data for the stacked model
stacked_train <- stacked_data[stacked_train_rows, ] # Store the training data
stacked_test <- stacked_data[-stacked_train_rows, ] # Store the testing data

# Let's do a quick check that random split worked (using dependent variable)
summary(stacked_train$actual)
summary(stacked_test$actual)
# The mean value is similar between the train and test data sets signifying the split was done successfully

# Build and predict a decision tree model as a model stacked on top the other five models 

# Build Model (without weights)
# stacked_unweighted_model <- C5.0(as.factor(actual) ~ ., data = stacked_train)
# saveRDS(stacked_unweighted_model, "stackedJobModel.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

stacked_unweighted_model <- readRDS("stackedJobModel.RDS")
plot(stacked_unweighted_model)

# Build Model (with weights)
stacked_cost_matrix <- matrix(c(0, 1, 5, 0), nrow = 2) 

stacked_cost_matrix # Check the matrix looks correct 

# stacked_model <- C5.0(as.factor(actual) ~ ., data = stacked_train, costs = stacked_cost_matrix)
# saveRDS(stacked_model, "stackedJobModel_2.RDS")

# Since I have saved the model above, I can comment out the code above so that the model does not re-run each time I knit my file

stacked_model <- readRDS("stackedJobModel_2.RDS")
plot(stacked_model)

# Predict (without weights)
stacked_unweighted_pred <- predict(stacked_unweighted_model, stacked_test)

# Predict (with weights)
stacked_pred <- predict(stacked_model, stacked_test)
```

## Step 6: Evaluate Model
Now that the models are created, we can evaluate them by creating confusion matrices. It will be important to look at the accuracy and sensitivity of the model. We also want to make sure that we am minimizing false negatives, as those are much more costly that false positives. [explain what false negatives and false positives are and why false negatives are worse]

```{r class.source = 'fold-hide', message=FALSE}
# Let's build some confusion matrices

library(caret) # We need this library to build a confusion matrix

library(knitr) # Load in library so that the table is formatted in an easy to read manner
```

### Logistic Regression Model
```{r}
cm_lr <- confusionMatrix(as.factor(lr_bin_pred), as.factor(job_test$fraudulent), positive = "1")
cm_lr
```

### Decision Tree Model
```{r}
# Decision Tree Model (without weights)
cm_unweighted_dt <- confusionMatrix(as.factor(dt_pred), as.factor(job_test$fraudulent), positive = "1")
# Looking at the confusion matrix, we need to apply a cost matrix. In this situation, the false negatives are extremely costly. As such, we want to apply a cost matrix that weights the false negatives appropriately. I will apply a cost matrix that costs false negatives at 6:1 ratio to false positives to reduce the number of false negatives. However, this will increase the number of false positives. However, we are not as concerned about this as it is less costly to deal with jobs posts that are falsely flagged than fraudulent posts that are missed. 
cm_unweighted_dt

# Decision Tree Model (with weights)
cm_dt <- confusionMatrix(as.factor(dt_weights_pred), as.factor(job_test$fraudulent), positive = "1")
cm_dt
```

### SVM Models
```{r}
cm_SVM <- confusionMatrix(as.factor(SVM_bin_prob), as.factor(job_test$fraudulent), positive = "1")
cm_SVM

cm_SVM_2 <- confusionMatrix(as.factor(SVM_bin_prob_2), as.factor(job_test$fraudulent), positive = "1")
cm_SVM_2
```

### Random Forest
```{r}
cm_rf <- confusionMatrix(as.factor(rf_pred), as.factor(job_test$fraudulent), positive = "1")
cm_rf
```

### KNN Model
```{r}
cm_KNN <- confusionMatrix(as.factor(KNN_bin_prob), as.factor(job_scaled_test[,35]), positive = "1")
cm_KNN
```

### ANN Model
```{r}
cm_ANN <- confusionMatrix(as.factor(ANN_bin_pred), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN

cm_ANN_2 <- confusionMatrix(as.factor(ANN_bin_pred_2), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN_2

cm_ANN_3 <- confusionMatrix(as.factor(ANN_bin_pred_3), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN_3

cm_ANN_4 <- confusionMatrix(as.factor(ANN_bin_pred_4), as.factor(job_scaled_test$fraudulent), positive = "1")
cm_ANN_4
```

#### ANN Model Comparison
```{r class.source = 'fold-hide'}
ANN_Comparison <- data.frame(
  "Model" = c("ANN_1", "ANN_2", "ANN_3", "ANN_4"),
  "Accuracy" = c(round(cm_ANN$overall["Accuracy"], 4), round(cm_ANN_2$overall["Accuracy"], 4), round(cm_ANN_3$overall["Accuracy"], 4), round(cm_ANN_4$overall["Accuracy"], 4)),
  "Sensitivity" = c(round(cm_ANN$byClass["Sensitivity"], 4), round(cm_ANN_2$byClass["Sensitivity"], 4), round(cm_ANN_3$byClass["Sensitivity"], 4), round(cm_ANN_4$byClass["Sensitivity"], 4)),
  "Kappa" = c(round(cm_ANN$overall["Kappa"], 4), round(cm_ANN_2$overall["Kappa"], 4), round(cm_ANN_3$overall["Kappa"], 4), round(cm_ANN_4$overall["Kappa"])),
  "P-Value" = c(round(cm_ANN$overall["AccuracyPValue"], 4), round(cm_ANN_2$overall["AccuracyPValue"], 4), round(cm_ANN_3$overall["AccuracyPValue"], 4), round(cm_ANN_4$overall["AccuracyPValue"], 4))
)

kable(ANN_Comparison, format = "markdown")
```

Looking at these 4 models, I will use **ANN_3**, as it has the highest sensitivity of `r round(cm_ANN_3$byClass["Sensitivity"], 4)`. However, this higher sensitivity does come at a trade-off of lower accuracy. I will use this model for the implementation step, along with feeding it into the stacked model.


### Stacked Model
```{r}
# Raw data values
cm_unweight_stacked <- confusionMatrix(as.factor(stacked_unweighted_pred), as.factor(stacked_test$actual), positive = "1")
# Looking at the confusion matrix, we need to apply a cost matrix. In this situation, the false negatives are extremely costly. As such, we want to apply a cost matrix that weights the false negatives appropriately. I will apply a cost matrix that costs false negatives at 5:1 ratio to false positives to reduce the number of false negatives. However, this will increase the number of false positives. However, we are not as concerned about this as it is less costly to deal with jobs posts that are falsely flagged than fraudulent posts that are missed. 
cm_unweight_stacked

cm_stacked <- confusionMatrix(as.factor(stacked_pred), as.factor(stacked_test$actual), positive = "1")
cm_stacked
```
### Model Comparison
```{r class.source = 'fold-hide'}
Model_Comparison <- data.frame(
  "Model" = c("Logistic Regression", "Decision Tree (weights)", "SVM", "SVM_2", "Random Forest", "KNN", "ANN", "Stacked Model"),
  "Accuracy" = c(round(cm_lr$overall["Accuracy"], 4), 
                 round(cm_dt$overall["Accuracy"], 4), 
                 round(cm_SVM$overall["Accuracy"], 4), 
                 round(cm_SVM_2$overall["Accuracy"], 4), 
                 round(cm_rf$overall["Accuracy"], 4), 
                 round(cm_KNN$overall["Accuracy"], 4), 
                 round(cm_ANN$overall["Accuracy"], 4), 
                 round(cm_stacked$overall["Accuracy"], 4)),
  "Sensitivity" = c(round(cm_lr$byClass["Sensitivity"], 4), 
                    round(cm_dt$byClass["Sensitivity"], 4), 
                    round(cm_SVM$byClass["Sensitivity"], 4), 
                    round(cm_SVM_2$byClass["Sensitivity"], 4),
                    round(cm_rf$byClass["Sensitivity"], 4), 
                    round(cm_KNN$byClass["Sensitivity"], 4), 
                    round(cm_ANN$byClass["Sensitivity"], 4), 
                    round(cm_stacked$byClass["Sensitivity"], 4)),
  "Kappa" = c(round(cm_lr$overall["Kappa"], 4), 
              round(cm_dt$overall["Kappa"], 4), 
              round(cm_SVM$overall["Kappa"], 4), 
              round(cm_SVM_2$overall["Kappa"], 4), 
              round(cm_rf$overall["Kappa"], 4), 
              round(cm_KNN$overall["Kappa"], 4), 
              round(cm_ANN$overall["Kappa"], 4), 
              round(cm_stacked$overall["Kappa"], 4)),
  "P-Value" = c(round(cm_lr$overall["AccuracyPValue"], 4), 
                round(cm_dt$overall["AccuracyPValue"], 4), 
                round(cm_SVM$overall["AccuracyPValue"], 4), 
                round(cm_SVM_2$overall["AccuracyPValue"], 4), 
                round(cm_rf$overall["AccuracyPValue"], 4), 
                round(cm_KNN$overall["AccuracyPValue"], 4), 
                round(cm_ANN$overall["AccuracyPValue"], 4), 
                round(cm_stacked$overall["AccuracyPValue"], 4))
)

kable(Model_Comparison, format = "markdown")
```

* The **Random Forest** model has the highest accuracy
* The **Stacked** model has the highest sensitivity
* The **Random Forest** model has the highest kappa
* The **KNN** model has the smallest p-value

**Stacked Model** <br>
Comparing the stacked model to the individual models, the stacked model has the third highest accuracy (`r  round(cm_stacked$overall["Accuracy"], 4)`). It does have the highest sensitivity (`r round(cm_stacked$byClass["Sensitivity"], 4)`). It has the second highest kappa (just behind the Random Forest model). Lastly, while it does not have the smallest p-value, it does have a p-value of `r round(cm_stacked$overall["AccuracyPValue"], 4)`, showing the model is significant.

## Step 7: Implement Model
Now that the models are created and evaluated, it is time to implement the models and see the financial impacts. It is important to also calculate the financial impact of having no model. I make assumptions (below) for the financial data I am missing. 

### Assumptions

* There are no costs associated with true negatives and true positives
* For **false positives**, these are job postings that our model flags as fraudulent that are actually real. As a result, the company would then have then had to submit a request to get their posting unflagged and placed back on the sight. This would cost a total of **$35**.
  * This would cost $20 in processing the unflagging request
  * This would "cost" $15 in reputation hurt to job posting site as companies do not want to have to post on sites that they have to worry about falsely getting flagged as fraudulent on. 
* For **false negatives**, these are job postings that our model does not flag as fraudulent but actually are. As a result, the site users who apply to this posting risk their time being wasted or even worse, their information being stolen. This would cost a total of **$500**. 
  * This would cost $100 in conducting a formal investigation to remove this post (after the fact) and then investigate any other fraudulent posts related to this on. 
  * This would "cost" $400 in harm to the many hundreds of applicants who applied. If there were 500 applications, it would be about $0.80 per applicant. 
* Being able to identify fraudulent posts increase confidence in the sight. As a result, more companies will post jobs and more people will use the site to find and apply to jobs. This is valued at **`fraud_identification_rate` * $1,000,000**

*Note: Results will be scaled up to 100,000 posts so (they are comparable)*


```{r class.source = 'fold-hide'}
# Assumptions
fp_cost <- 35
fn_cost <- 500
num_posts = 100000
nm_scalar = num_posts/nrow(job)
m_scalar = num_posts/nrow(job_test)
sm_scalar = num_posts/nrow(stacked_test)
bonus <- 1000000
```


#### No Model
```{r class.source = 'fold-hide'}
nm_frad = sum(job$fraudulent) * nm_scalar
nm_total_cost = nm_frad * fn_cost
```
With no model, there would be no way of flagging fraudulent posts ahead of time. As a result, we end up treating all posts as non-fraudulent. Therefore, we miss **`r format(round(nm_frad, 0), big.mark = ",")` (all) fraudulent posts**, costing **\$`r format(round(nm_total_cost, 2), big.mark = ",")`**.

#### Logistic Regression Model
```{r class.source = 'fold-hide'}
lr_fp_cost = fp_cost * cm_lr$table["1", "0"] * m_scalar
lr_fn_cost = fn_cost * cm_lr$table["0", "1"] * m_scalar
lr_suc_rate = (cm_lr$table["1", "1"] * m_scalar) / nm_frad
lr_bonus = bonus * lr_suc_rate
lr_total_cost = lr_fp_cost + lr_fn_cost - lr_bonus
```
With a model, we ended up with **`r format(round(cm_lr$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(lr_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_lr$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(lr_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(lr_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(lr_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(lr_total_cost, 2), big.mark = ",")`**.

#### Decision Tree Model
```{r class.source = 'fold-hide'}
dt_fp_cost = fp_cost * cm_dt$table["1", "0"] * m_scalar
dt_fn_cost = fn_cost * cm_dt$table["0", "1"] * m_scalar
dt_suc_rate = (cm_dt$table["1", "1"] * m_scalar) / nm_frad
dt_bonus = bonus * dt_suc_rate
dt_total_cost = dt_fp_cost + dt_fn_cost - dt_bonus
```
With a model, we ended up with **`r format(round(cm_dt$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(dt_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_dt$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(dt_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(dt_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(dt_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(dt_total_cost, 2), big.mark = ",")`**.

#### SVM Model
```{r class.source = 'fold-hide'}
SVM_fp_cost = fp_cost * cm_SVM$table["1", "0"] * m_scalar
SVM_fn_cost = fn_cost * cm_SVM$table["0", "1"] * m_scalar
SVM_suc_rate = (cm_SVM$table["1", "1"] * m_scalar) / nm_frad
SVM_bonus = bonus * SVM_suc_rate
SVM_total_cost = SVM_fp_cost + SVM_fn_cost - SVM_bonus
```
With a model, we ended up with **`r format(round(cm_SVM$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(SVM_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_SVM$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(SVM_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(SVM_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(SVM_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(SVM_total_cost, 2), big.mark = ",")`**.

#### SVM Model 2
```{r class.source = 'fold-hide'}
SVM_fp_cost_2 = fp_cost * cm_SVM_2$table["1", "0"] * m_scalar
SVM_fn_cost_2 = fn_cost * cm_SVM_2$table["0", "1"] * m_scalar
SVM_suc_rate_2 = (cm_SVM_2$table["1", "1"] * m_scalar) / nm_frad
SVM_bonus_2 = bonus * SVM_suc_rate_2
SVM_total_cost_2 = SVM_fp_cost_2 + SVM_fn_cost_2 - SVM_bonus_2
```
With a model, we ended up with **`r format(round(cm_SVM_2$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(SVM_fp_cost_2, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_SVM_2$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(SVM_fn_cost_2, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(SVM_suc_rate_2, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(SVM_bonus_2, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(SVM_total_cost_2, 2), big.mark = ",")`**.

#### Random Forest
```{r class.source = 'fold-hide'}
rf_fp_cost = fp_cost * cm_rf$table["1", "0"] * m_scalar
rf_fn_cost = fn_cost * cm_rf$table["0", "1"] * m_scalar
rf_suc_rate = (cm_rf$table["1", "1"] * m_scalar) / nm_frad
rf_bonus = bonus * rf_suc_rate
rf_total_cost = rf_fp_cost + rf_fn_cost - rf_bonus
```
With a model, we ended up with **`r format(round(cm_rf$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(rf_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_rf$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(rf_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(rf_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(rf_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(rf_total_cost, 2), big.mark = ",")`**.

#### KNN Model
```{r class.source = 'fold-hide'}
KNN_fp_cost = fp_cost * cm_KNN$table["1", "0"] * m_scalar
KNN_fn_cost = fn_cost * cm_KNN$table["0", "1"] * m_scalar
KNN_suc_rate = (cm_KNN$table["1", "1"] * m_scalar) / nm_frad
KNN_bonus = bonus * KNN_suc_rate
KNN_total_cost = KNN_fp_cost + KNN_fn_cost - KNN_bonus
```
With a model, we ended up with **`r format(round(cm_KNN$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(KNN_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_KNN$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(KNN_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(KNN_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(KNN_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(KNN_total_cost, 2), big.mark = ",")`**.

#### ANN Model
```{r class.source = 'fold-hide'}
ANN_fp_cost = fp_cost * cm_ANN_3$table["1", "0"] * m_scalar
ANN_fn_cost = fn_cost * cm_ANN_3$table["0", "1"] * m_scalar
ANN_suc_rate = (cm_ANN$table["1", "1"] * m_scalar) / nm_frad
ANN_bonus = bonus * ANN_suc_rate
ANN_total_cost = ANN_fp_cost + ANN_fn_cost - ANN_bonus
```
With a model, we ended up with **`r format(round(cm_ANN_3$table["1", "0"] * m_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(ANN_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_ANN_3$table["0", "1"] * m_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(ANN_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(ANN_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(ANN_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(ANN_total_cost, 2), big.mark = ",")`**.

#### Stacked Model
```{r class.source = 'fold-hide'}
stacked_fp_cost = fp_cost * cm_stacked$table["1", "0"] * sm_scalar
stacked_fn_cost = fn_cost * cm_stacked$table["0", "1"] * sm_scalar
stacked_suc_rate = (cm_stacked$table["1", "1"] * sm_scalar) / nm_frad
stacked_bonus = bonus * stacked_suc_rate
stacked_total_cost = stacked_fp_cost + stacked_fn_cost - stacked_bonus
```
With a model, we ended up with **`r format(round(cm_stacked$table["1", "0"] * sm_scalar, 0), big.mark = ",")` false positives**, costing **\$`r format(round(stacked_fp_cost, 2), big.mark = ",")`**. 
We ended up with **`r format(round(cm_stacked$table["0", "1"] * sm_scalar, 0), big.mark = ",")` false negatives**, costing **\$`r format(round(stacked_fn_cost, 2), big.mark = ",")`**. 
This model has a fraud success identification rate of **`r format(round(stacked_suc_rate, 2), big.mark = ",")`**, resulting in a benefit of **\$`r format(round(stacked_bonus, 2), big.mark = ",")`**. 
So, the total cost the job posting site company incurs is **\$`r format(round(stacked_total_cost, 2), big.mark = ",")`**.

### Results
```{r class.source = 'fold-hide'}
results <- data.frame(
  "Model" = c("No Model", "Logistic Regression", "Decision Tree", "SVM", "SVM_2", "Random Forest", "KNN", "ANN", "Stacked Model"),
  "Accuracy" = c(0, round(cm_lr$overall["Accuracy"], 4), 
                 round(cm_dt$overall["Accuracy"], 4), 
                 round(cm_SVM$overall["Accuracy"], 4), 
                 round(cm_SVM_2$overall["Accuracy"], 4), 
                 round(cm_rf$overall["Accuracy"], 4), 
                 round(cm_KNN$overall["Accuracy"], 4), 
                 round(cm_ANN$overall["Accuracy"], 4), 
                 round(cm_stacked$overall["Accuracy"], 4)),
  "Sensitivity" = c(0, round(cm_lr$byClass["Sensitivity"], 4), 
                    round(cm_dt$byClass["Sensitivity"], 4), 
                    round(cm_SVM$byClass["Sensitivity"], 4), 
                    round(cm_SVM_2$byClass["Sensitivity"], 4), 
                    round(cm_rf$byClass["Sensitivity"], 4), 
                    round(cm_KNN$byClass["Sensitivity"], 4), 
                    round(cm_ANN$byClass["Sensitivity"], 4), round(cm_stacked$byClass["Sensitivity"], 4))
)
results$FP_Cost <- c(0, lr_fp_cost, dt_fp_cost, SVM_fp_cost, SVM_fp_cost_2, rf_fp_cost, KNN_fp_cost, ANN_fp_cost, stacked_fp_cost)
results$FN_Cost <- c(nm_total_cost, lr_fn_cost, dt_fn_cost, SVM_fn_cost, SVM_fn_cost_2, rf_fn_cost, KNN_fn_cost, ANN_fn_cost, stacked_fn_cost)
results$Benefit <- c(0, lr_bonus, dt_bonus, SVM_bonus, SVM_bonus_2, rf_bonus, KNN_bonus, ANN_bonus, stacked_bonus)
results$Total_Cost <- c(nm_total_cost, lr_total_cost, dt_total_cost, SVM_total_cost, SVM_total_cost_2, rf_total_cost, KNN_total_cost, ANN_total_cost, stacked_total_cost)
results$Cost_Savings <- (nm_total_cost - results$Total_Cost)

results$FP_Cost <- format(round(results$FP_Cost, 2), big.mark = ",")
results$FN_Cost <- format(round(results$FN_Cost, 2), big.mark = ",")
results$Benefit <- format(round(results$Benefit, 2), big.mark = ",")
results$Total_Cost <- format(round(results$Total_Cost, 2), big.mark = ",")
results$Cost_Savings <- format(round(results$Cost_Savings, 2), big.mark = ",")

kable(results, format = "markdown", digits = 4)
```

The **stacked** model is the best model. 

### Conclusion 

